[
  {
    "objectID": "planning-for-submission.html",
    "href": "planning-for-submission.html",
    "title": "Planning for submission",
    "section": "",
    "text": "You’ve done your experiment, extracted some DNA or RNA, and want to sequence it, but now what?\n\n\nYour DNA or RNA samples will need to pass certain quality cut-offs for sequencing, which the sequencing facility will ask for and will have thresholds they require upon submission (e.g., see the Otago Genomics Facility requirements here). After you have done your extractions in the lab, you should perform QC with:\n\nA spectrophotometer (e.g., NanoDrop), which will give you a good indication of the purity/contamination of your sample through 260/280 and 260/230 ratios. The NanoDrop is not a very reliable instrument for concentration of your nucleic acid.\nA fluorescent-based quantification method (e.g, Qubit), which will give you a very accurate indication of concentration of your nucleic acid (but not of any contaminants)\nA fragment analysis instrument (e.g., Agilent 2100 Bioanalyzer, Agilent 5300 Fragment Analyzer or Agilent TapeStation), which you can think of like a high tech agarose gel. It will give you a good indication of your 28S/18S ratio for RNA (this is used to determine the overall RIN - RNA integrity number) or your fragment size distribution for DNA.\n\nTogether, these will give you an almost complete picture of your sample, ready for sequencing. Note that if your samples are below threshold quality, there are protocols that can allow for lower quality or degraded samples. Ideally, this should only be done if there is no option to re-extract, re-purify or repeat the experiment.\n\n\n\nThe first thing that needs to be done for Illumina, PacBio or Oxford Nanopore (ONT) sequencing is to turn the DNA or RNA sample into something called a library. This converts the raw nucleic acids into a form that the sequencer can actually read. This is generally done by the technician that will also sequence your samples, but some labs may do library prep in-house (i.e., you may do it yourself!). Nanopore is a little different, in that it can sequence native RNA directly, with minimal prep before sequencing. DNA library preparation for ONT sequencing is also generally simpler than for Illumina and PacBio. This is because the technology used to sequence the DNA/RNA using ONT is quite different to how Illumina and PacBio achieve it–more on that in the next section on flow cells and sequencing platforms 101\n\n\n\n\n\n\nIllumina lib prep molecular workflow diagram\n\n\n\n\n\n\n\n\n\nLibrary preparation (for Illumina and PacBio) generally involves the following steps, and takes 1-2 days in the lab:\n\n(Optional) Target enrichment / RNA selection\nSelectively enriching for your target molecule (e.g., for standard RNAseq you likely will do polyA capture).\n\n(RNAseq only) Converting your RNA into cDNA using reverse transcriptase.\n\nFragmentation (if needed) Some kits fragment RNA before reverse transcription, others fragment cDNA/DNA after.\nThis ensures fragments are the right size for the platform (e.g., ~200–400 bp for Illumina).\nEnd repair and A-tailing\nEnds are cleaned up so adapters can be ligated efficiently.\nAdapter ligation\nAdapters are short sequences that enable library binding to the flow cell (Illumina) or capture for SMRTbells (PacBio).\nSome kits combine this with indexing.\n\n\n\n\n\n\n\nPacBio lib prep - SMRTbell adapters\n\n\n\n\n\n\nNote: the SMRTbell technology allows HiFi (high fidelity) long read sequencing. The DNA becomes circularised, which allows the polymerase to make repeated passes around the DNA and the consensus sequence therefore has a higher accuracy than single pass sequencing.\n\n\n\n\nIndexing (barcoding)\nIndices allow multiple samples to be pooled together and sequenced on the same flow cell, then computationally separated afterward (called mulitplexing and demultiplexing).\nSize selection / cleanup\nTypically done with magnetic beads to remove adapter dimers and select the desired fragment range.\nLibrary amplification (if required)\nSome protocols use PCR to enrich adapter-ligated molecules; others (e.g., some PacBio) are PCR-free.\nFinal QC and quantification\nUsing Qubit, Bioanalyzer/Tapestation/Fragment Analyzer, etc. This step ensures your library meets sequencing requirements.\n\n\n\n\n\n\n\nBioanalyzer trace of final library (Illumina mRNA lib prep)\n\n\n\n\n\n\n\n\n\nThere are different library prep methods (i.e., protocols) for each platform that you will also need to chose. This will depend on a few things, such as:\n\nThe quality of your RNA/DNA (e.g., high-quality RNA allows polyA selection; degraded samples may require ribo-depletion or specialised kits).\n\nThe species/tissue type you extracted your RNA/DNA from (e.g., plants have rRNA types that require plant-specific depletion kits, some tissues have high mitochondrial RNA content).\n\nThe type of analysis you want to do i.e., what is your research question\n\nFor example, if you are doing a ‘standard’ Illumina RNA sequencing project (e.g,. you plan to do differential gene expression analysis to compare different samples), a common choice is Illumina stranded mRNA library prep, which uses polyA selection to capture mRNA. However, you may chose Illumina total RNA library prep with ribo-depletion, which is more expensive, but it has some advantages such as: it can capture non-polyadenylated RNAs (more comprehensive RNA profile) and and is a better option if your samples are partially degraded (it can also handle FFPE samples).\n\n\n\n\n\n\n\n\nWhy do you think polyA capture is used for ‘standard’ RNAseq?\n\n\n\n\n\nMature mRNAs have polyA tails, which can be selectively isolated using oligo DT coated beads that bind mature RNAs only. All other non-polyadenylated nucleic acids and cellular debris can then be washed away.\n\n\n\n\n\n\n\n\n\nWhy do you think we bother adding “index” sequences to libraries at all? Why not sequence each sample separately?\n\n\n\n\n\nIndexing (barcoding) allows multiple samples to be pooled in a single sequencing run. This massively reduces cost and time. Because each library carries a unique index, the sequencer can mix them together, and downstream software can computationally separate (demultiplex) them accurately afterward.\n\n\n\n\n\n\n\n\n\nWhy do you think size selection is such an important step? What would happen if we skipped it?\n\n\n\n\n\nSize selection ensures that fragments fall within the size range the sequencer expects. Without it, you may get:\n\nadapter dimers (which waste sequencing reads as they take up ‘real estate’ on the flow cell)\ntoo-short fragments (which cluster preferentially, causing over-representation and distorts the data e.g., sequencing may read into adapters or flow cell)\n\ntoo-long fragments (which may not fully sequence or reduce yield)\n\n\n\n\n\n\n\n\nThe three major sequencing platform and consumables companies are Illumina, PacBio (Pacific Biosystems) and Oxford Nanopore Technologies (ONT). Each make different platforms/instruments that can handle different levels of throughput, but the chemistry and the ‘reading’ of the sequencing is the main point of difference between the three companies.\nA few examples of the different platforms are:\nIllumina\n\nMiSeq\n\nNextSeq\n\nNovaSeq\n\nPacBio\n\nSequel II\n\nRevio\n\nOxford Nanopore Technologies\n\nMinION\n\nGridION\n\nPromethION\n\nA flow cell is the physical surface inside the sequencing machine (or platform) where the actual reading of DNA or RNA occurs. There are different sizes you can chose from, depending on how many reads you need. Although Illumina, PacBio, and ONT all call their consumables “flow cells,” the underlying technologies are very different.\nFor the most part, and in-depth knowledge of how these flow cells work is not needed to get you started with your sequencing project.\nHere are the basic differences:\n\n\n\n\n\n\n\n\n\nFeature\nIllumina\nPacBio (HiFi / SMRT)\nOxford Nanopore (ONT)\n\n\n\n\nHow sequencing works\nSequencing-by-synthesis (fluorescent nucleotides added one base at a time)\nSingle-molecule real-time sequencing (polymerase incorporates fluorescent bases inside ZMWs)\nNanopore sensing (changes in ionic current as DNA/RNA passes through a pore)\n\n\nFlow cell structure\nPatterned flow cell with billions of oligos that form clonal clusters\nSMRT Cell containing millions of Zero-Mode Waveguides (ZMWs)\nMembrane embedded with thousands of protein nanopores\n\n\nWhat binds to the flow cell\nLibraries bind via adapters to oligos → amplified into clusters\nA single SMRTbell + polymerase complex loads into each ZMW\nDNA or RNA strand with a motor protein threads into a nanopore\n\n\nSignal detected\nFluorescent signal imaged each cycle\nFluorescent flashes when each base is incorporated\nChanges in electrical current across the pore\n\n\nAmplification?\nYes — cluster generation required\nNo — true single-molecule reads\nNo (PCR-free), though can use PCR in library prep\n\n\nTypical read length\n100–300 bp\n10–25 kb HiFi reads\n10 kb to &gt;100 kb (ultra-long &gt;1 Mb possible)\n\n\nCan sequence native RNA?\nNo — convert to cDNA library\nNo — convert to cDNA library\nYes — direct RNA sequencing\n\n\nStrengths\nHigh accuracy, high throughput, cost-efficient, chemistry highly compatible with different species/tissues\nHighly accurate long reads; excellent for haplotype resolution\nUltra-long reads; portable; real-time analysis; can detect base mods\n\n\nLimitations\nShort reads only\nLower throughput than Illumina; expensive\nHigher raw error rate; pore lifetime limits yield and susceptible to clogging\n\n\nBest used for\nStandard RNA-seq (DGE); de novo transcriptomes; high-depth short-read assays; metagenomics; error-correcting long reads\nGenome assembly (chromosome-level with HiFi); full-length RNA (Iso-Seq for isoforms); structural variant detection\nField-based sequencing (e.g, rapid microbial identification); genome assembly; native RNA including modification detection (e.g., methylation)\n\n\n\n\n\n\nChoosing a platform to do your sequencing comes down to the question you are trying to answer. See the last row in the table above ‘Best use for’ for some examples of why you might pick one platform over another!\nWhen you get in contact with a sequencing facility, the question they will ask you is not how many samples are you sequencing, but rather, how many reads do you need? This will determine what size flow cell you need, and to some extent, which platform you will use, as different platforms have different capacity (e.g,. Illumina NextSeq is a ‘medium throughput’ platform, NovaSeq is a ‘large throughput’ platform). The number of reads you need scale with the size of the genome and/or complexity of your transcriptome.\nAs a general rule of thumb, for transcriptome sequencing you will need:\nTable 1: Reads per sample\n\n\n\nPurpose / Type\nApprox reads per sample\n\n\n\n\nGene expression profiling\n5–25 million\n\n\nComplete expression + alternative splicing\n30–60+ million\n\n\nDe novo transcriptome assembly\n~100+ million\n\n\n\n\n\nThe next thing the sequencing facility will ask you if you are doing short-read sequencing (Illumina) is do you want single-end or paired-end reads. This refers to whether you want a single read (read 1), sequenced from only one end of the library molecule (fragment), or if you want two reads per library molecule (read 1 and read 2, antisense and sense strands). Paired-end costs more, but gives you more resolution.\nThere are pros and cons to choosing either chemistry:\nTable 2: Single-end vs paired-end chemistry (short-read Illumina sequencing)\n\n\n\n\n\n\n\n\nChemistry\nPros\nCons\n\n\n\n\nSingle-end (SE)\nLower cost, fewer reads required (may be able to do more samples); sufficient for basic gene-level DGE\nLimited splice/isoform resolution; less confident mapping when mapping to a genome\n\n\nPaired-end (PE)\nBetter alignment; improved splice junction and isoform detection; more robust for complex transcriptomes or de novo transcriptome assembly\nHigher cost; ~2× sequencing required (two reads per fragment)\n\n\n\n\n\nLastly, if you are doing short-read sequencing (i.e., Illumina), the sequencing facility will ask you what read length you want. You can typically chose from between 50bp-300bp (platform-dependent). The choice between a lower or higher read length will be a balance of cost (higher read length = higher cost) and the level of information you need (complex, novel or de novo transcriptomes require higher read lengths; more straight forward analyses with well-annotated genomes can utilise lower read lengths). In contrast, for long-read sequencing platforms (ONT and PacBio), you do not specify a fixed read length. Instead, reads are generated as single, continuous sequences, and their length is determined by the size of the input molecules, the library preparation method, and the sequencing chemistry.\nChoosing a read length is a trade-off between cost, and the amount of information you will recover:\nTable 3: Read length (short-read Illumina sequencing)\n\n\n\n\n\n\n\n\nRead length\nPros\nCons\n\n\n\n\n50 bp\nLowest cost; highest sample multiplexing; sufficient for basic gene-level DGE in well-annotated genomes\nPoor isoform and splice junction resolution; higher multi-mapping\n\n\n100 bp\nGood balance of cost and information; reliable splice junction detection; widely used for standard RNA-seq\nSlightly lower throughput than 50 bp; may miss very complex isoforms\n\n\n150 bp\nImproved isoform resolution; better mapping across repetitive regions; useful for novel transcript discovery\nHigher cost; fewer reads per run\n\n\n300 bp\nMaximum per-read information; helpful for de novo transcriptome assembly\nRarely necessary for Illumina RNA-seq; expensive; reduced throughput\n\n\n\n\n\nExample RNAseq scenario:\nYou are working on a mouse model of cancer genomics. You want to do differential gene expression analysis to compare tumour samples to non-cancerous control tissue, to see if you can find genes that are up or down regulated in the cancerous tissue. You have 40 RNA samples, and since your species has a high-quality reference genome and annotation (i.e., mouse: Mus musculus), you know you have a “good” genome assembly and annotations to map your sequencing data back to (more on “good” genomes later!). The mouse genome is ~2.7 Gb (= 2,700,000,000 bp), diploid, with ~20,000 protein-coding genes.\nYou are particularly interested in alternative splicing and novel splice junctions, as you suspect that cancer-associated genes are often regulated at the isoform level.\nYou will sequence your samples at the Otago Genomics Facility, and see on their website they have an Illumina MiSeq and an Illumina NextSeq 2000.\n\n\n\n\n\n\nWhich platform will you chose?\n\n\n\n\n\nThe Illumina NextSeq 2000 is a ‘medium’ sized short read sequencing platform, ideal for standard RNAseq, and is well-suited to this project. It can output up to 540Gb.\nThe Illumina MiSeq is a ‘small’ sized short read sequencing platform, better suited to QC or amplicon sequencing, and can output up to 30Gb. It is too small for this project.\nIllumina benchtop sequencing platforms comparison\n\n\n\nNext you need to decide how many reads you need.\nBased on table 1 above, how many reads do you need per sample?\n\n\n\n\n\n\nReads per sample choice:\n\n\n\n\n\nYou decide you need 30 million reads per sample. You pick the lower end of the scale, as you suspect the genes you are most interested in will be highly expressed, and don’t expect your tissue to have a particular high transcript diversity that would require more reads.\nYou work out what minimum output you need from the flow cell:\n30 mil reads * 40 samples = 1200 million reads (1.2B).\n\n\n\nNow you need to decide if you want paired-end or single end reads.\nBased on table 2 above, what would you pick?\n\n\n\n\n\n\nPaired-end vs single end choice:\n\n\n\n\n\nPaired-end sequencing will give better detection of novel isoforms and splice junctions.\nThis doubles the number of reads generated per fragment and will give you better resolution.\n\n\n\nYou now need to decide which read length you need. Given the mouse genome is well-annotated, but you are looking for potentially novel isoforms, which read length would you pick, based on table 3?\n\n\n\n\n\n\nRead length choice:\n\n\n\n\n\n50 bp would be a good choice for a well-annotated genome like mouse for standard RNAseq, but does not suit this experiment, as you are looking for novel isoforms/alternative splicing.\n100bp is probably the best choice, balancing cost with novel isoform discovery.\nYou could also chose 150bp, if you want to be sure you’d capture novel isoforms, especially if they are quite long or complex genes - and don’t mind a higher cost.\n\n\n\nYou are now ready to pick your flow cell. Check out the Illumina NextSeq2000 flow cell specifications and chose which flow cell will suit this project best. There are four flow cells in ascending output size you can choose from: P1, P2, P3 and P4.\n\n\n\n\n\n\nWhich flow cell will you pick?\n\n\n\n\n\nBased on paired end sequencing using 100bp read length (i.e., 2 x 100bp):\nTotal reads needed for your experiment: 1.2 billion reads.\nCalculation: 1.2 billion reads x 100 (bp) x 2 (PE) = 240 Gb (240 gigabase pairs, or 240 billion individual DNA bases sequenced).\n\nP1 → No 100bp option and way too low\n\nP2 → 80 Gb (too low for 1.2B demand)\n\nP3 → 240 Gb (fits 1.2B reads requirement exactly - best choice!)\n\nP4 → 360 Gb (oversized for this project)\n\nNote: the maximum output stated for the flow cell is under optimal conditions, so the P3 flow cell choice just fits, but it is possible you will have less reads then anticipated (e.g., instead of 30 mil reads per sample, you may get 28 mil per sample).\n\n\n\nCongratulations! You are now ready to sequence your RNAseq samples.\n \nNow let’s look at DNA sequencing in more detail.\nDNA sequencing may refer to whole genome, whole exome, or targeted sequencing approaches. FILL in a bit more here by someone who knows DNAseq better\nFor Genome sequencing you will need:\n\n\n\n\n\n\n\n\n\nGenome size\nExample species\nApprox genome size\nApprox reads per sample\n\n\n\n\nTiny\nVirus\n&lt;0.1 Mb\n0.1–0.5 million\n\n\nSmall\nBacteria\n5 Mb\n5–10 million\n\n\nMedium\nYeast\n12 Mb\n10–20 million\n\n\nLarge\nFruit fly (D. melanogaster)\n175 Mb\n50–100 million\n\n\nVery Large\nHuman\n3 Gb\n600–1,200 million\n\n\nHuge\nWheat\n16 Gb\n6–12 billion\n\n\n\nWhere Mb = megabase pairs (i.e., 1 Mb = 1,000,000 bp)\nCoverage. talking about average coverage - really repetitive regions can have very low covergae, other more easily resolved sections will have higher coverage.\nheterozygosity homozygosity.\nhaplotypes etc.\nStick mostly to decision points - not a huge lecture/tutorial on genome/ DNA biology. Assume learners haev a genetics background - they just have not yet translated that knowledge into a practical application of NGS.\n\n\nExample DNAseq scenario:\nYou are want to de novo assemble the genome of the New Zealand swamp maire (Syzygium maire), a critically-endangered, endemic myrtaceae species, which is under threat from the pathogen myrtle rust.\nBased on genome sizes of related Syzygium and Myrtaceae species (e.g., Syzygium aromaticum ~370 Mb, S. grande ~405 Mb), you expect the S. maire genome to be on the order of ~350–400 Mb.\nDe novo genome assembly requires long read sequencing to resolve the longest contiguous sequences possible (ideally the full chromosome length, but that is often not easy or possible to achieve!).\n\n\n\n\n\n\nWhich long-read sequencing platform should you chose?\n\n\n\n\n\nThere are two long-read platforms – PacBio and ONT. PacBio better for larger genomes than ONT? Ask someone who knows this to fill in.\n\n\n\nBalkwill et al. 2024. Tree Genetics & Genomes.\nThe authors also used Illumina sequencing in this paper. What did they use it for? Why do you think they chose to do Illumina sequencing for these samples rather than PacBio?\n\n\n\n\n\n\nSolutions:\n\n\n\n\n\nAnswer: 30 x samples at low coverage for resequencing.\nAnswer: cost, accuracy at SNP level. dont need to resolve full genomes as that was not their question.\n\n\n\n\nNote 1: Batch variation. Are you doing all your samples in one batch, or will you have multiple batches? Technical variation can occur, so you may want to wait and do all samples at once on one larger flow cell, or make sure you randomise samples across sequencing batches.\n\n\nNote 2: DNAseq vs RNAseq. You may have noticed we call it ‘RNAseq’, even though we convert the RNA into DNA before sequencing! By convention, this is still called RNAseq, to differentiate it from true DNAseq.\n\n\n\n\n\nCurrent as of: December 2025\n\nSequencing facilities which offer NGS services:\nOtago Genomics Facility (OGF)\nLocation: The University of Otago, Biochemistry Building.\n\nIllumina NextSeq 2000\n\nIllumina MiSeq\nONT P2 Solo\n\nONT MinION\nNanoString nCounter Analysis System (non-NGS)\n\nPerforms: Moderate to large scale RNAseq, DNAseq, amplicon seq etc\nMassey Genome Service (MGS)\nLocation: Massey University\n\n2 x Illumina MiSeq\n\nApplied Biosystems 3500xl capillary instrumentation (non-NGS).\n\nPerforms: small scale RNA or DNAseq, single gene (Sanger sequencing)\nAuckland Genomics\n\nplatforms: Illumina and ONT. They don’t list on their website which ones they have.\n\nProcotocol available - dont want to go into too much detail but becoems another decision point. Massey offer TruSeq lib prep. Auckland uses the newer kit. (slightly higher input required than the newer chemistry, both are very robust options. Newer one is supposed to be faster and better for more degarded RNA. Higher multiplexing for new kit 384 vs 96 for truseq).\nSee here for comparison between Illumina TruSeq stranded and Illumina stranded mRNA kits\nNon-service platforms around New Zealand:\nAgResearch\n- GenomNZ https://www.agresearch.co.nz/products-and-services/genomnz/ (Invermay) have a NovaSeq - they will do it if they have time and you ask nicely. They are a commercial animal DNA genotyping laboratory.\nLincoln\n- have MGI - compatible with Illumina libraries. not on website.\nother\nCustom science (supplier, does not do the sequencing for you)\n\nNew PacBio Revio in Auckland\n\n\n\nstuff on submission process, show example form ?\nhow do you get data back from them ? how do they transfer it?",
    "crumbs": [
      "Planning for submission"
    ]
  },
  {
    "objectID": "planning-for-submission.html#quality-control-of-dnarna",
    "href": "planning-for-submission.html#quality-control-of-dnarna",
    "title": "Planning for submission",
    "section": "",
    "text": "Your DNA or RNA samples will need to pass certain quality cut-offs for sequencing, which the sequencing facility will ask for and will have thresholds they require upon submission (e.g., see the Otago Genomics Facility requirements here). After you have done your extractions in the lab, you should perform QC with:\n\nA spectrophotometer (e.g., NanoDrop), which will give you a good indication of the purity/contamination of your sample through 260/280 and 260/230 ratios. The NanoDrop is not a very reliable instrument for concentration of your nucleic acid.\nA fluorescent-based quantification method (e.g, Qubit), which will give you a very accurate indication of concentration of your nucleic acid (but not of any contaminants)\nA fragment analysis instrument (e.g., Agilent 2100 Bioanalyzer, Agilent 5300 Fragment Analyzer or Agilent TapeStation), which you can think of like a high tech agarose gel. It will give you a good indication of your 28S/18S ratio for RNA (this is used to determine the overall RIN - RNA integrity number) or your fragment size distribution for DNA.\n\nTogether, these will give you an almost complete picture of your sample, ready for sequencing. Note that if your samples are below threshold quality, there are protocols that can allow for lower quality or degraded samples. Ideally, this should only be done if there is no option to re-extract, re-purify or repeat the experiment.",
    "crumbs": [
      "Planning for submission"
    ]
  },
  {
    "objectID": "planning-for-submission.html#sequencing-libraries",
    "href": "planning-for-submission.html#sequencing-libraries",
    "title": "Planning for submission",
    "section": "",
    "text": "The first thing that needs to be done for Illumina, PacBio or Oxford Nanopore (ONT) sequencing is to turn the DNA or RNA sample into something called a library. This converts the raw nucleic acids into a form that the sequencer can actually read. This is generally done by the technician that will also sequence your samples, but some labs may do library prep in-house (i.e., you may do it yourself!). Nanopore is a little different, in that it can sequence native RNA directly, with minimal prep before sequencing. DNA library preparation for ONT sequencing is also generally simpler than for Illumina and PacBio. This is because the technology used to sequence the DNA/RNA using ONT is quite different to how Illumina and PacBio achieve it–more on that in the next section on flow cells and sequencing platforms 101\n\n\n\n\n\n\nIllumina lib prep molecular workflow diagram\n\n\n\n\n\n\n\n\n\nLibrary preparation (for Illumina and PacBio) generally involves the following steps, and takes 1-2 days in the lab:\n\n(Optional) Target enrichment / RNA selection\nSelectively enriching for your target molecule (e.g., for standard RNAseq you likely will do polyA capture).\n\n(RNAseq only) Converting your RNA into cDNA using reverse transcriptase.\n\nFragmentation (if needed) Some kits fragment RNA before reverse transcription, others fragment cDNA/DNA after.\nThis ensures fragments are the right size for the platform (e.g., ~200–400 bp for Illumina).\nEnd repair and A-tailing\nEnds are cleaned up so adapters can be ligated efficiently.\nAdapter ligation\nAdapters are short sequences that enable library binding to the flow cell (Illumina) or capture for SMRTbells (PacBio).\nSome kits combine this with indexing.\n\n\n\n\n\n\n\nPacBio lib prep - SMRTbell adapters\n\n\n\n\n\n\nNote: the SMRTbell technology allows HiFi (high fidelity) long read sequencing. The DNA becomes circularised, which allows the polymerase to make repeated passes around the DNA and the consensus sequence therefore has a higher accuracy than single pass sequencing.\n\n\n\n\nIndexing (barcoding)\nIndices allow multiple samples to be pooled together and sequenced on the same flow cell, then computationally separated afterward (called mulitplexing and demultiplexing).\nSize selection / cleanup\nTypically done with magnetic beads to remove adapter dimers and select the desired fragment range.\nLibrary amplification (if required)\nSome protocols use PCR to enrich adapter-ligated molecules; others (e.g., some PacBio) are PCR-free.\nFinal QC and quantification\nUsing Qubit, Bioanalyzer/Tapestation/Fragment Analyzer, etc. This step ensures your library meets sequencing requirements.\n\n\n\n\n\n\n\nBioanalyzer trace of final library (Illumina mRNA lib prep)\n\n\n\n\n\n\n\n\n\nThere are different library prep methods (i.e., protocols) for each platform that you will also need to chose. This will depend on a few things, such as:\n\nThe quality of your RNA/DNA (e.g., high-quality RNA allows polyA selection; degraded samples may require ribo-depletion or specialised kits).\n\nThe species/tissue type you extracted your RNA/DNA from (e.g., plants have rRNA types that require plant-specific depletion kits, some tissues have high mitochondrial RNA content).\n\nThe type of analysis you want to do i.e., what is your research question\n\nFor example, if you are doing a ‘standard’ Illumina RNA sequencing project (e.g,. you plan to do differential gene expression analysis to compare different samples), a common choice is Illumina stranded mRNA library prep, which uses polyA selection to capture mRNA. However, you may chose Illumina total RNA library prep with ribo-depletion, which is more expensive, but it has some advantages such as: it can capture non-polyadenylated RNAs (more comprehensive RNA profile) and and is a better option if your samples are partially degraded (it can also handle FFPE samples).\n\n\n\n\n\n\n\n\nWhy do you think polyA capture is used for ‘standard’ RNAseq?\n\n\n\n\n\nMature mRNAs have polyA tails, which can be selectively isolated using oligo DT coated beads that bind mature RNAs only. All other non-polyadenylated nucleic acids and cellular debris can then be washed away.\n\n\n\n\n\n\n\n\n\nWhy do you think we bother adding “index” sequences to libraries at all? Why not sequence each sample separately?\n\n\n\n\n\nIndexing (barcoding) allows multiple samples to be pooled in a single sequencing run. This massively reduces cost and time. Because each library carries a unique index, the sequencer can mix them together, and downstream software can computationally separate (demultiplex) them accurately afterward.\n\n\n\n\n\n\n\n\n\nWhy do you think size selection is such an important step? What would happen if we skipped it?\n\n\n\n\n\nSize selection ensures that fragments fall within the size range the sequencer expects. Without it, you may get:\n\nadapter dimers (which waste sequencing reads as they take up ‘real estate’ on the flow cell)\ntoo-short fragments (which cluster preferentially, causing over-representation and distorts the data e.g., sequencing may read into adapters or flow cell)\n\ntoo-long fragments (which may not fully sequence or reduce yield)",
    "crumbs": [
      "Planning for submission"
    ]
  },
  {
    "objectID": "planning-for-submission.html#flow-cells-and-sequencing-platforms-101",
    "href": "planning-for-submission.html#flow-cells-and-sequencing-platforms-101",
    "title": "Planning for submission",
    "section": "",
    "text": "The three major sequencing platform and consumables companies are Illumina, PacBio (Pacific Biosystems) and Oxford Nanopore Technologies (ONT). Each make different platforms/instruments that can handle different levels of throughput, but the chemistry and the ‘reading’ of the sequencing is the main point of difference between the three companies.\nA few examples of the different platforms are:\nIllumina\n\nMiSeq\n\nNextSeq\n\nNovaSeq\n\nPacBio\n\nSequel II\n\nRevio\n\nOxford Nanopore Technologies\n\nMinION\n\nGridION\n\nPromethION\n\nA flow cell is the physical surface inside the sequencing machine (or platform) where the actual reading of DNA or RNA occurs. There are different sizes you can chose from, depending on how many reads you need. Although Illumina, PacBio, and ONT all call their consumables “flow cells,” the underlying technologies are very different.\nFor the most part, and in-depth knowledge of how these flow cells work is not needed to get you started with your sequencing project.\nHere are the basic differences:\n\n\n\n\n\n\n\n\n\nFeature\nIllumina\nPacBio (HiFi / SMRT)\nOxford Nanopore (ONT)\n\n\n\n\nHow sequencing works\nSequencing-by-synthesis (fluorescent nucleotides added one base at a time)\nSingle-molecule real-time sequencing (polymerase incorporates fluorescent bases inside ZMWs)\nNanopore sensing (changes in ionic current as DNA/RNA passes through a pore)\n\n\nFlow cell structure\nPatterned flow cell with billions of oligos that form clonal clusters\nSMRT Cell containing millions of Zero-Mode Waveguides (ZMWs)\nMembrane embedded with thousands of protein nanopores\n\n\nWhat binds to the flow cell\nLibraries bind via adapters to oligos → amplified into clusters\nA single SMRTbell + polymerase complex loads into each ZMW\nDNA or RNA strand with a motor protein threads into a nanopore\n\n\nSignal detected\nFluorescent signal imaged each cycle\nFluorescent flashes when each base is incorporated\nChanges in electrical current across the pore\n\n\nAmplification?\nYes — cluster generation required\nNo — true single-molecule reads\nNo (PCR-free), though can use PCR in library prep\n\n\nTypical read length\n100–300 bp\n10–25 kb HiFi reads\n10 kb to &gt;100 kb (ultra-long &gt;1 Mb possible)\n\n\nCan sequence native RNA?\nNo — convert to cDNA library\nNo — convert to cDNA library\nYes — direct RNA sequencing\n\n\nStrengths\nHigh accuracy, high throughput, cost-efficient, chemistry highly compatible with different species/tissues\nHighly accurate long reads; excellent for haplotype resolution\nUltra-long reads; portable; real-time analysis; can detect base mods\n\n\nLimitations\nShort reads only\nLower throughput than Illumina; expensive\nHigher raw error rate; pore lifetime limits yield and susceptible to clogging\n\n\nBest used for\nStandard RNA-seq (DGE); de novo transcriptomes; high-depth short-read assays; metagenomics; error-correcting long reads\nGenome assembly (chromosome-level with HiFi); full-length RNA (Iso-Seq for isoforms); structural variant detection\nField-based sequencing (e.g, rapid microbial identification); genome assembly; native RNA including modification detection (e.g., methylation)",
    "crumbs": [
      "Planning for submission"
    ]
  },
  {
    "objectID": "planning-for-submission.html#decision-points",
    "href": "planning-for-submission.html#decision-points",
    "title": "Planning for submission",
    "section": "",
    "text": "Choosing a platform to do your sequencing comes down to the question you are trying to answer. See the last row in the table above ‘Best use for’ for some examples of why you might pick one platform over another!\nWhen you get in contact with a sequencing facility, the question they will ask you is not how many samples are you sequencing, but rather, how many reads do you need? This will determine what size flow cell you need, and to some extent, which platform you will use, as different platforms have different capacity (e.g,. Illumina NextSeq is a ‘medium throughput’ platform, NovaSeq is a ‘large throughput’ platform). The number of reads you need scale with the size of the genome and/or complexity of your transcriptome.\nAs a general rule of thumb, for transcriptome sequencing you will need:\nTable 1: Reads per sample\n\n\n\nPurpose / Type\nApprox reads per sample\n\n\n\n\nGene expression profiling\n5–25 million\n\n\nComplete expression + alternative splicing\n30–60+ million\n\n\nDe novo transcriptome assembly\n~100+ million\n\n\n\n\n\nThe next thing the sequencing facility will ask you if you are doing short-read sequencing (Illumina) is do you want single-end or paired-end reads. This refers to whether you want a single read (read 1), sequenced from only one end of the library molecule (fragment), or if you want two reads per library molecule (read 1 and read 2, antisense and sense strands). Paired-end costs more, but gives you more resolution.\nThere are pros and cons to choosing either chemistry:\nTable 2: Single-end vs paired-end chemistry (short-read Illumina sequencing)\n\n\n\n\n\n\n\n\nChemistry\nPros\nCons\n\n\n\n\nSingle-end (SE)\nLower cost, fewer reads required (may be able to do more samples); sufficient for basic gene-level DGE\nLimited splice/isoform resolution; less confident mapping when mapping to a genome\n\n\nPaired-end (PE)\nBetter alignment; improved splice junction and isoform detection; more robust for complex transcriptomes or de novo transcriptome assembly\nHigher cost; ~2× sequencing required (two reads per fragment)\n\n\n\n\n\nLastly, if you are doing short-read sequencing (i.e., Illumina), the sequencing facility will ask you what read length you want. You can typically chose from between 50bp-300bp (platform-dependent). The choice between a lower or higher read length will be a balance of cost (higher read length = higher cost) and the level of information you need (complex, novel or de novo transcriptomes require higher read lengths; more straight forward analyses with well-annotated genomes can utilise lower read lengths). In contrast, for long-read sequencing platforms (ONT and PacBio), you do not specify a fixed read length. Instead, reads are generated as single, continuous sequences, and their length is determined by the size of the input molecules, the library preparation method, and the sequencing chemistry.\nChoosing a read length is a trade-off between cost, and the amount of information you will recover:\nTable 3: Read length (short-read Illumina sequencing)\n\n\n\n\n\n\n\n\nRead length\nPros\nCons\n\n\n\n\n50 bp\nLowest cost; highest sample multiplexing; sufficient for basic gene-level DGE in well-annotated genomes\nPoor isoform and splice junction resolution; higher multi-mapping\n\n\n100 bp\nGood balance of cost and information; reliable splice junction detection; widely used for standard RNA-seq\nSlightly lower throughput than 50 bp; may miss very complex isoforms\n\n\n150 bp\nImproved isoform resolution; better mapping across repetitive regions; useful for novel transcript discovery\nHigher cost; fewer reads per run\n\n\n300 bp\nMaximum per-read information; helpful for de novo transcriptome assembly\nRarely necessary for Illumina RNA-seq; expensive; reduced throughput\n\n\n\n\n\nExample RNAseq scenario:\nYou are working on a mouse model of cancer genomics. You want to do differential gene expression analysis to compare tumour samples to non-cancerous control tissue, to see if you can find genes that are up or down regulated in the cancerous tissue. You have 40 RNA samples, and since your species has a high-quality reference genome and annotation (i.e., mouse: Mus musculus), you know you have a “good” genome assembly and annotations to map your sequencing data back to (more on “good” genomes later!). The mouse genome is ~2.7 Gb (= 2,700,000,000 bp), diploid, with ~20,000 protein-coding genes.\nYou are particularly interested in alternative splicing and novel splice junctions, as you suspect that cancer-associated genes are often regulated at the isoform level.\nYou will sequence your samples at the Otago Genomics Facility, and see on their website they have an Illumina MiSeq and an Illumina NextSeq 2000.\n\n\n\n\n\n\nWhich platform will you chose?\n\n\n\n\n\nThe Illumina NextSeq 2000 is a ‘medium’ sized short read sequencing platform, ideal for standard RNAseq, and is well-suited to this project. It can output up to 540Gb.\nThe Illumina MiSeq is a ‘small’ sized short read sequencing platform, better suited to QC or amplicon sequencing, and can output up to 30Gb. It is too small for this project.\nIllumina benchtop sequencing platforms comparison\n\n\n\nNext you need to decide how many reads you need.\nBased on table 1 above, how many reads do you need per sample?\n\n\n\n\n\n\nReads per sample choice:\n\n\n\n\n\nYou decide you need 30 million reads per sample. You pick the lower end of the scale, as you suspect the genes you are most interested in will be highly expressed, and don’t expect your tissue to have a particular high transcript diversity that would require more reads.\nYou work out what minimum output you need from the flow cell:\n30 mil reads * 40 samples = 1200 million reads (1.2B).\n\n\n\nNow you need to decide if you want paired-end or single end reads.\nBased on table 2 above, what would you pick?\n\n\n\n\n\n\nPaired-end vs single end choice:\n\n\n\n\n\nPaired-end sequencing will give better detection of novel isoforms and splice junctions.\nThis doubles the number of reads generated per fragment and will give you better resolution.\n\n\n\nYou now need to decide which read length you need. Given the mouse genome is well-annotated, but you are looking for potentially novel isoforms, which read length would you pick, based on table 3?\n\n\n\n\n\n\nRead length choice:\n\n\n\n\n\n50 bp would be a good choice for a well-annotated genome like mouse for standard RNAseq, but does not suit this experiment, as you are looking for novel isoforms/alternative splicing.\n100bp is probably the best choice, balancing cost with novel isoform discovery.\nYou could also chose 150bp, if you want to be sure you’d capture novel isoforms, especially if they are quite long or complex genes - and don’t mind a higher cost.\n\n\n\nYou are now ready to pick your flow cell. Check out the Illumina NextSeq2000 flow cell specifications and chose which flow cell will suit this project best. There are four flow cells in ascending output size you can choose from: P1, P2, P3 and P4.\n\n\n\n\n\n\nWhich flow cell will you pick?\n\n\n\n\n\nBased on paired end sequencing using 100bp read length (i.e., 2 x 100bp):\nTotal reads needed for your experiment: 1.2 billion reads.\nCalculation: 1.2 billion reads x 100 (bp) x 2 (PE) = 240 Gb (240 gigabase pairs, or 240 billion individual DNA bases sequenced).\n\nP1 → No 100bp option and way too low\n\nP2 → 80 Gb (too low for 1.2B demand)\n\nP3 → 240 Gb (fits 1.2B reads requirement exactly - best choice!)\n\nP4 → 360 Gb (oversized for this project)\n\nNote: the maximum output stated for the flow cell is under optimal conditions, so the P3 flow cell choice just fits, but it is possible you will have less reads then anticipated (e.g., instead of 30 mil reads per sample, you may get 28 mil per sample).\n\n\n\nCongratulations! You are now ready to sequence your RNAseq samples.\n \nNow let’s look at DNA sequencing in more detail.\nDNA sequencing may refer to whole genome, whole exome, or targeted sequencing approaches. FILL in a bit more here by someone who knows DNAseq better\nFor Genome sequencing you will need:\n\n\n\n\n\n\n\n\n\nGenome size\nExample species\nApprox genome size\nApprox reads per sample\n\n\n\n\nTiny\nVirus\n&lt;0.1 Mb\n0.1–0.5 million\n\n\nSmall\nBacteria\n5 Mb\n5–10 million\n\n\nMedium\nYeast\n12 Mb\n10–20 million\n\n\nLarge\nFruit fly (D. melanogaster)\n175 Mb\n50–100 million\n\n\nVery Large\nHuman\n3 Gb\n600–1,200 million\n\n\nHuge\nWheat\n16 Gb\n6–12 billion\n\n\n\nWhere Mb = megabase pairs (i.e., 1 Mb = 1,000,000 bp)\nCoverage. talking about average coverage - really repetitive regions can have very low covergae, other more easily resolved sections will have higher coverage.\nheterozygosity homozygosity.\nhaplotypes etc.\nStick mostly to decision points - not a huge lecture/tutorial on genome/ DNA biology. Assume learners haev a genetics background - they just have not yet translated that knowledge into a practical application of NGS.\n\n\nExample DNAseq scenario:\nYou are want to de novo assemble the genome of the New Zealand swamp maire (Syzygium maire), a critically-endangered, endemic myrtaceae species, which is under threat from the pathogen myrtle rust.\nBased on genome sizes of related Syzygium and Myrtaceae species (e.g., Syzygium aromaticum ~370 Mb, S. grande ~405 Mb), you expect the S. maire genome to be on the order of ~350–400 Mb.\nDe novo genome assembly requires long read sequencing to resolve the longest contiguous sequences possible (ideally the full chromosome length, but that is often not easy or possible to achieve!).\n\n\n\n\n\n\nWhich long-read sequencing platform should you chose?\n\n\n\n\n\nThere are two long-read platforms – PacBio and ONT. PacBio better for larger genomes than ONT? Ask someone who knows this to fill in.\n\n\n\nBalkwill et al. 2024. Tree Genetics & Genomes.\nThe authors also used Illumina sequencing in this paper. What did they use it for? Why do you think they chose to do Illumina sequencing for these samples rather than PacBio?\n\n\n\n\n\n\nSolutions:\n\n\n\n\n\nAnswer: 30 x samples at low coverage for resequencing.\nAnswer: cost, accuracy at SNP level. dont need to resolve full genomes as that was not their question.\n\n\n\n\nNote 1: Batch variation. Are you doing all your samples in one batch, or will you have multiple batches? Technical variation can occur, so you may want to wait and do all samples at once on one larger flow cell, or make sure you randomise samples across sequencing batches.\n\n\nNote 2: DNAseq vs RNAseq. You may have noticed we call it ‘RNAseq’, even though we convert the RNA into DNA before sequencing! By convention, this is still called RNAseq, to differentiate it from true DNAseq.",
    "crumbs": [
      "Planning for submission"
    ]
  },
  {
    "objectID": "planning-for-submission.html#nz-sequencing-facilities-and-services",
    "href": "planning-for-submission.html#nz-sequencing-facilities-and-services",
    "title": "Planning for submission",
    "section": "",
    "text": "Current as of: December 2025\n\nSequencing facilities which offer NGS services:\nOtago Genomics Facility (OGF)\nLocation: The University of Otago, Biochemistry Building.\n\nIllumina NextSeq 2000\n\nIllumina MiSeq\nONT P2 Solo\n\nONT MinION\nNanoString nCounter Analysis System (non-NGS)\n\nPerforms: Moderate to large scale RNAseq, DNAseq, amplicon seq etc\nMassey Genome Service (MGS)\nLocation: Massey University\n\n2 x Illumina MiSeq\n\nApplied Biosystems 3500xl capillary instrumentation (non-NGS).\n\nPerforms: small scale RNA or DNAseq, single gene (Sanger sequencing)\nAuckland Genomics\n\nplatforms: Illumina and ONT. They don’t list on their website which ones they have.\n\nProcotocol available - dont want to go into too much detail but becoems another decision point. Massey offer TruSeq lib prep. Auckland uses the newer kit. (slightly higher input required than the newer chemistry, both are very robust options. Newer one is supposed to be faster and better for more degarded RNA. Higher multiplexing for new kit 384 vs 96 for truseq).\nSee here for comparison between Illumina TruSeq stranded and Illumina stranded mRNA kits\nNon-service platforms around New Zealand:\nAgResearch\n- GenomNZ https://www.agresearch.co.nz/products-and-services/genomnz/ (Invermay) have a NovaSeq - they will do it if they have time and you ask nicely. They are a commercial animal DNA genotyping laboratory.\nLincoln\n- have MGI - compatible with Illumina libraries. not on website.\nother\nCustom science (supplier, does not do the sequencing for you)\n\nNew PacBio Revio in Auckland\n\n\n\nstuff on submission process, show example form ?\nhow do you get data back from them ? how do they transfer it?",
    "crumbs": [
      "Planning for submission"
    ]
  },
  {
    "objectID": "data-storage-and-management.html",
    "href": "data-storage-and-management.html",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "Your samples have been sequenced, now what?\n\n\nGenomic data files are huge, and for one project you will often have several hundred gigabytes (GB) to sometimes terabytes (TB) of data (more on the actual genomic file types in the next section Data wrangling and processing). You most likely won’t be able to store and analyse these on your local computer, or transfer the files in the ‘traditional’ way using USB drives, so you’ll need to make use of a high capacity storage (HCS), high performance computing (HPC), and file transfer protocols (FTP). An introduction to genomics is also an introduction to the world of high performance computing!\nCaveat: small genomes such as bacteria often can be transferred, stored and analysed on your local computer.\nYou have a few options of how you want to store and analyse your data, which may be through HPC/HCS services offered by your university or institute, or through the national computing and digital network REANNZ.\n\n\n\nREANNZ is a Crown-owned membership organisation that powers Aotearoa’s research and education network. Launched in 2007, their high-performance national digital network (or NREN) helps members collaborate and contribute to data-intensive and complex science and research initiatives – both here in New Zealand and across the globe.\nThey offer many products and services check out their website here, but here we will focus on their HPC platform that you may chose to use to analyse your data.\nProjects on REANNZ\nTo use the HPC services, you first need to make or be listed as a member on a ‘Project’. A ‘Project’ will have a code (e.g., nesi03181 or uoo00431) and will need to be linked to a funding source to charge back compute resources used. Multiple people can use one Project; this is often the preferable way to use a Project when collaborating on the same dataset or project. You’ll need to discuss whether you need your own Project, or can be added to an existing Project, with whoever is supplying the funding (most likely your PI / supervisor).\nEither way, first make your own account using your institutional details by logging in here: https://my.nesi.org.nz/.\n\nFrom there, you can apply/access/view your Projects, and manage the compute resource allocation of the Project.\nOnce you have an account and a Project, you can start using the HPC by logging in to Open OnDemand here: ondemand.nesi.org.nz\n\nOnce logged in, you’ll see an app-based dashboard.\nClick on ‘Jupyter Lab’ and then you can chose a few different settings before launching. For the most part, you will only need to change the Project Code and the number of hours you need the session open for. You can leave the number of cores and memory per job as the default lowest settings.\n\nOnce the session is open, you can interact with your files through the directory structure on the left, and open either a Terminal or various other scripting programs through the Launcher. More on using the Terminal (also known as shell / unix shell / bash) in the next lesson.\nFor each Project, you will have a nobackup and a project directory. Keep your raw files in the project directory and do your analysis in the nobackup directory. As you can guess by the name, this directory is not backed up, and is best used for working analyses. Additionally, files not modified after 90 days are auto deleted. Final analysed files and imprortant scripts should be copied into the project directory once you have completed your analysis. You may be surprised by how many intermediate files you make and how much ‘tinkering’ you do during your analyses–making use of the two directories helps keep your file system tidier.\n\nNote: It can be a good idea to back-up raw genomic files on a HCS–your inst\n\nGetting your genomic files on to the REANNZ cluster\nFILL IN HERE\n??no backup/project directories?\nWant more info? REANNZ provide extensive documentation on how to use their High Performance Computing (HPC) platforms.\n\nIn July 2025, the New Zealand eScience Infrastructure (NeSI) was integrated into REANNZ. You may have heard of NeSI before, but if not, it is good to be aware of the name, as you’ll see some legacy branding in the REANNZ documentation or log-in platform (e.g., my.nesi.org.nz is still in use).\n\n\n\n\nIf you are a University of Otago staff or student, you can use the “Aoraki” HPC cluster. Like REANNZ, you will also connect to the cluster through an Open OnDemand app-based interactive web page.\nhttps://rtis.cspages.otago.ac.nz/research-computing/cluster/index.html\n\n\n\nAuckland?\n\n\n\n\n\nIn 2016, the FAIR principles were published in Scientific Data. These guidelines set to provide a standard through which to improve digital assets:\n\nFindability\n\nAccessibility\n\nInteroperability\n\nReuse\n\nOpen science advocates have embraced this framework, while others critise its lack of protection of data, for not considering the rights and interest of those that should hold governance over data, in particular, indigenous peoples. In response to this, the CARE framework has also been proposed by Global Indigenous Data Analysis, to complement the existing FAIR principles:\n\nCollective benefit\n\nAuthority to control\n\nResponsibility\n\nEthics\n\n\nThis story “Open with care; Indigenous researchers and communities are reshaping how Western science thinks about data ownership” is an interesting read on these perspectives.\nThe Aotearoa Genomic Data Repository (AGDR) and Rakeiora platform are examples of services that have adopted these CARE principles.\n\n\n\nFrom the AGDR website:\n“The Aotearoa Genomic Data Repository provides secure within-nation storage, management and sharing of non-human genomic data generated from biological and environmental samples originating in Aotearoa New Zealand. This resource has been developed to follow the principles of Māori Data Sovereignty, and to enable kaitiakitanga (guardianship), so that iwi, hapū and whānau (tribes, kinship groups and families) can effectively exercise their responsibilities as guardians over biological entities that are taonga (precious or treasured). While the repository is designed to facilitate the sharing of data — making it findable by researchers and interoperable with data held in other genomic repositories — the decision-making process regarding who can access the data is entirely in the hands of those holding kaitiakitanga over each data set.””\nThe AGDR is enabled by MBIE funding to Genomics Aotearoa.\n\n\n\nThe Rakeiora Genomics Platform is designed to enable and test pilot precision medicine research, linking genomics and health.\nThey have chosen a walled garden approach to this infrastructure.\n“This means that all genomic data and health data, and its analysis, are undertaken only within this computational environment – the data never leave and are certainly never downloaded to a researcher’s own computer or a hospital clinical laboratory computer…\n…Its modular nature will allow linkage (with consent) to other health data systems currently under development in Aotearoa New Zealand. This approach in particular addresses transparency of use and control over narratives. Lack of these features in other precision medicine platforms worldwide has led to significant problems for patients and research participants, including indigenous communities; learning from these negative experiences is key to avoid repeating them.” – From the Genomics Aotearoa website.\n\n\n\n\nYou are probably familiar with using the NIH NCBI database, hosted by the United States Government, for things such as BLAST searches and literature searches.\nNCBI has a repository called the Sequence Read Archive (SRA), which is where a large proportion of genomic data from all over the world is stored, as publicly available raw, high-throughput, sequencing data.\nIf your data has no ethical concerns, this can be a great resource for making your data freely available as easily citeable in publications.\nHere we will not go into detail on how to either download or upload data to the SRA (see documentation on their website). But some points to be aware of are:\n\nYou may want to start with making a BioProject for your specific project, which will link all of your sequencing data together under one umbrella (as a general rule, 1 publication = 1 BioProject).\nYou’ll need to also make BioSample submission/s, which will be linked to the matching sequence data in the SRA, and all houses under one BioProject accession number. BioSamples describe the unique biological attributes of your sample (e.g, species, sex, age, location, tissue). This is often (but not strictly) 1 BioSample = 1 library.\n\n\nEqually, if you are downloading data from the SRA, you can make use of BioProject and BioSample accessions to identify and orientate yourself to the data.\nSHOW EXAMPLE screenshot of bipproject example, biosampler ecxample, SRA example.\nThis is not the only public repository of sequence reads, some other examples include: European Nucleotide Archive (ENA) and the DNA data bank of Japan DDBJ Sequence Read Archive (DRA),",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#genomic-data-files",
    "href": "data-storage-and-management.html#genomic-data-files",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "Genomic data files are huge, and for one project you will often have several hundred gigabytes (GB) to sometimes terabytes (TB) of data (more on the actual genomic file types in the next section Data wrangling and processing). You most likely won’t be able to store and analyse these on your local computer, or transfer the files in the ‘traditional’ way using USB drives, so you’ll need to make use of a high capacity storage (HCS), high performance computing (HPC), and file transfer protocols (FTP). An introduction to genomics is also an introduction to the world of high performance computing!\nCaveat: small genomes such as bacteria often can be transferred, stored and analysed on your local computer.\nYou have a few options of how you want to store and analyse your data, which may be through HPC/HCS services offered by your university or institute, or through the national computing and digital network REANNZ.",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#research-and-education-advanced-network-new-zealand-reannz",
    "href": "data-storage-and-management.html#research-and-education-advanced-network-new-zealand-reannz",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "REANNZ is a Crown-owned membership organisation that powers Aotearoa’s research and education network. Launched in 2007, their high-performance national digital network (or NREN) helps members collaborate and contribute to data-intensive and complex science and research initiatives – both here in New Zealand and across the globe.\nThey offer many products and services check out their website here, but here we will focus on their HPC platform that you may chose to use to analyse your data.\nProjects on REANNZ\nTo use the HPC services, you first need to make or be listed as a member on a ‘Project’. A ‘Project’ will have a code (e.g., nesi03181 or uoo00431) and will need to be linked to a funding source to charge back compute resources used. Multiple people can use one Project; this is often the preferable way to use a Project when collaborating on the same dataset or project. You’ll need to discuss whether you need your own Project, or can be added to an existing Project, with whoever is supplying the funding (most likely your PI / supervisor).\nEither way, first make your own account using your institutional details by logging in here: https://my.nesi.org.nz/.\n\nFrom there, you can apply/access/view your Projects, and manage the compute resource allocation of the Project.\nOnce you have an account and a Project, you can start using the HPC by logging in to Open OnDemand here: ondemand.nesi.org.nz\n\nOnce logged in, you’ll see an app-based dashboard.\nClick on ‘Jupyter Lab’ and then you can chose a few different settings before launching. For the most part, you will only need to change the Project Code and the number of hours you need the session open for. You can leave the number of cores and memory per job as the default lowest settings.\n\nOnce the session is open, you can interact with your files through the directory structure on the left, and open either a Terminal or various other scripting programs through the Launcher. More on using the Terminal (also known as shell / unix shell / bash) in the next lesson.\nFor each Project, you will have a nobackup and a project directory. Keep your raw files in the project directory and do your analysis in the nobackup directory. As you can guess by the name, this directory is not backed up, and is best used for working analyses. Additionally, files not modified after 90 days are auto deleted. Final analysed files and imprortant scripts should be copied into the project directory once you have completed your analysis. You may be surprised by how many intermediate files you make and how much ‘tinkering’ you do during your analyses–making use of the two directories helps keep your file system tidier.\n\nNote: It can be a good idea to back-up raw genomic files on a HCS–your inst\n\nGetting your genomic files on to the REANNZ cluster\nFILL IN HERE\n??no backup/project directories?\nWant more info? REANNZ provide extensive documentation on how to use their High Performance Computing (HPC) platforms.\n\nIn July 2025, the New Zealand eScience Infrastructure (NeSI) was integrated into REANNZ. You may have heard of NeSI before, but if not, it is good to be aware of the name, as you’ll see some legacy branding in the REANNZ documentation or log-in platform (e.g., my.nesi.org.nz is still in use).",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#aoraki-at-the-university-of-otago",
    "href": "data-storage-and-management.html#aoraki-at-the-university-of-otago",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "If you are a University of Otago staff or student, you can use the “Aoraki” HPC cluster. Like REANNZ, you will also connect to the cluster through an Open OnDemand app-based interactive web page.\nhttps://rtis.cspages.otago.ac.nz/research-computing/cluster/index.html",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#other-universities-hpchcs-options",
    "href": "data-storage-and-management.html#other-universities-hpchcs-options",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "Auckland?",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#ethical-data-management",
    "href": "data-storage-and-management.html#ethical-data-management",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "In 2016, the FAIR principles were published in Scientific Data. These guidelines set to provide a standard through which to improve digital assets:\n\nFindability\n\nAccessibility\n\nInteroperability\n\nReuse\n\nOpen science advocates have embraced this framework, while others critise its lack of protection of data, for not considering the rights and interest of those that should hold governance over data, in particular, indigenous peoples. In response to this, the CARE framework has also been proposed by Global Indigenous Data Analysis, to complement the existing FAIR principles:\n\nCollective benefit\n\nAuthority to control\n\nResponsibility\n\nEthics\n\n\nThis story “Open with care; Indigenous researchers and communities are reshaping how Western science thinks about data ownership” is an interesting read on these perspectives.\nThe Aotearoa Genomic Data Repository (AGDR) and Rakeiora platform are examples of services that have adopted these CARE principles.\n\n\n\nFrom the AGDR website:\n“The Aotearoa Genomic Data Repository provides secure within-nation storage, management and sharing of non-human genomic data generated from biological and environmental samples originating in Aotearoa New Zealand. This resource has been developed to follow the principles of Māori Data Sovereignty, and to enable kaitiakitanga (guardianship), so that iwi, hapū and whānau (tribes, kinship groups and families) can effectively exercise their responsibilities as guardians over biological entities that are taonga (precious or treasured). While the repository is designed to facilitate the sharing of data — making it findable by researchers and interoperable with data held in other genomic repositories — the decision-making process regarding who can access the data is entirely in the hands of those holding kaitiakitanga over each data set.””\nThe AGDR is enabled by MBIE funding to Genomics Aotearoa.\n\n\n\nThe Rakeiora Genomics Platform is designed to enable and test pilot precision medicine research, linking genomics and health.\nThey have chosen a walled garden approach to this infrastructure.\n“This means that all genomic data and health data, and its analysis, are undertaken only within this computational environment – the data never leave and are certainly never downloaded to a researcher’s own computer or a hospital clinical laboratory computer…\n…Its modular nature will allow linkage (with consent) to other health data systems currently under development in Aotearoa New Zealand. This approach in particular addresses transparency of use and control over narratives. Lack of these features in other precision medicine platforms worldwide has led to significant problems for patients and research participants, including indigenous communities; learning from these negative experiences is key to avoid repeating them.” – From the Genomics Aotearoa website.",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#public-data",
    "href": "data-storage-and-management.html#public-data",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "You are probably familiar with using the NIH NCBI database, hosted by the United States Government, for things such as BLAST searches and literature searches.\nNCBI has a repository called the Sequence Read Archive (SRA), which is where a large proportion of genomic data from all over the world is stored, as publicly available raw, high-throughput, sequencing data.\nIf your data has no ethical concerns, this can be a great resource for making your data freely available as easily citeable in publications.\nHere we will not go into detail on how to either download or upload data to the SRA (see documentation on their website). But some points to be aware of are:\n\nYou may want to start with making a BioProject for your specific project, which will link all of your sequencing data together under one umbrella (as a general rule, 1 publication = 1 BioProject).\nYou’ll need to also make BioSample submission/s, which will be linked to the matching sequence data in the SRA, and all houses under one BioProject accession number. BioSamples describe the unique biological attributes of your sample (e.g, species, sex, age, location, tissue). This is often (but not strictly) 1 BioSample = 1 library.\n\n\nEqually, if you are downloading data from the SRA, you can make use of BioProject and BioSample accessions to identify and orientate yourself to the data.\nSHOW EXAMPLE screenshot of bipproject example, biosampler ecxample, SRA example.\nThis is not the only public repository of sequence reads, some other examples include: European Nucleotide Archive (ENA) and the DNA data bank of Japan DDBJ Sequence Read Archive (DRA),",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "organisation-tidiness.html",
    "href": "organisation-tidiness.html",
    "title": "Organisation and tidy data",
    "section": "",
    "text": "Good data organisation is the foundation of any research project. It not only sets you up well for an analysis, but it also makes it easier to come back to the project later and share with collaborators, including your most important collaborator - future you.\nOrganising a project that includes sequencing involves many components. There’s the experimental setup and conditions metadata, measurements of experimental parameters, sequencing preparation and sample information, the sequences themselves and the files and workflow of any bioinformatics analysis. So much of the information of a sequencing project is digital, and we need to keep track of our digital records in the same way we have a lab notebook and sample freezer. In this lesson, we’ll go through the project organisation and documentation that will make an efficient bioinformatics workflow possible. Not only will this make you a more effective bioinformatics researcher, it also prepares your data and project for publication, as grant agencies and publishers increasingly require this information.\n\n\nWhen we think about the data for a sequencing project, we often start by thinking about the sequencing data that we get back from the sequencing centre. However, equally or more important is the data you’ve generated about the sequences before it ever goes to the sequencing centre. This is the data about the data, often called the metadata. Without the information about what you sequenced, the sequence data itself is useless.\n\n\n\n\n\n\nEXERCISE 🧠🏋️‍♀️ - 3 mins\n\n\n\nWhat kind of data and information have you generated (or think you will generate) about your samples that can be considered metadata? How do you store this information?\nDiscuss with the group.\n\n\n\n\n\n\nSOLUTIONS\n\n\n\n\n\n\nExperimental conditions describing the biological samples, such as: temperature, weight, length, treatment.\nLaboratory/technical data, such as: protocol or kit used to extract DNA/RNA, specific processing equipment, concentrations or quality indicators (e.g., Nanodrop, Qubit, or Bioanalyzer results)\n\nLab notebook notes about how you conducted those experiments.\n\nSpreadsheet or tabular data with the data from your experiment and whatever you were measuring for your study.\n\n\n\n\n\n\n\n\n\nRegardless of the type of data you’re collecting, there are standard ways to enter that data into the spreadsheet to make it easier to analyse later. We often enter data in a way that makes it easy for us as humans to read and work with it, because we’re human! Computers need data structured in a way that they can use it. So to use this data in a computational workflow, we need to think like computers when we use spreadsheets.\nThe cardinal rules of using spreadsheet programs for data:\n\nLeave the raw data raw - do not change it!\n\nPut each observation or sample in its own row.\n\nPut all your variables in columns - the thing that vary between samples, like ‘strain’ or ‘DNA-concentration’.\n\nHave column names be explanatory, but without spaces. Use ‘-’, ’_’ or camel case instead of a space. For instance ‘library-prep-method’ or ‘LibraryPrep’is better than ’library preparation method’ or ‘prep’, because computers interpret spaces in particular ways.\n\nDo not combine multiple pieces of information in one cell. Sometimes it just seems like one thing, but think if that’s the only way you’ll want to be able to use or sort that data. For example, instead of having a column with species and strain name (e.g. E. coli K12) you would have one column with the species name (E. coli) and another with the strain name (K12). Depending on the type of analysis you want to do, you may even separate the genus and species names into distinct columns.\n\nExport the cleaned data to a text-based format like CSV (comma-separated values) format. This ensures that anyone can use the data, and is required by most data repositories.\n\n\n\n\n\n\n\n\nEXERCISE 🧠🏋️‍♀️ – 2 mins\n\n\n\nAbove is some potential spreadsheet data generated about a sequencing experiment.\nDiscuss some of the problems with the spreadsheet data shown above. You can look at the image, or download the file to your computer via this link and open it in Excel.\n\n\n\n\n\n\nSOLUTIONS\n\n\n\n\n\nHere is a clean version of the data (download link here). Note the following changes to make the data tidy:\n\nSections reordered to be in single columns\n\nRemoved formatting/colours which won’t be interpreted by most computational tools\n\nHeader information about the reference, facility, read length etc moved to their own columns\n\nSpaces replaced in column names with _\nStandarised language for mutator and cit columns i.e., \"+\" became plus\n\nData has also been saved as a tsv file rather than excel format.\n\n\n\n\n\n\n\n\n\nBest ways to name columns:\n\nBe careful of zero values versus missing values:",
    "crumbs": [
      "Organisation and tidy data"
    ]
  },
  {
    "objectID": "organisation-tidiness.html#metadata",
    "href": "organisation-tidiness.html#metadata",
    "title": "Organisation and tidy data",
    "section": "",
    "text": "When we think about the data for a sequencing project, we often start by thinking about the sequencing data that we get back from the sequencing centre. However, equally or more important is the data you’ve generated about the sequences before it ever goes to the sequencing centre. This is the data about the data, often called the metadata. Without the information about what you sequenced, the sequence data itself is useless.\n\n\n\n\n\n\nEXERCISE 🧠🏋️‍♀️ - 3 mins\n\n\n\nWhat kind of data and information have you generated (or think you will generate) about your samples that can be considered metadata? How do you store this information?\nDiscuss with the group.\n\n\n\n\n\n\nSOLUTIONS\n\n\n\n\n\n\nExperimental conditions describing the biological samples, such as: temperature, weight, length, treatment.\nLaboratory/technical data, such as: protocol or kit used to extract DNA/RNA, specific processing equipment, concentrations or quality indicators (e.g., Nanodrop, Qubit, or Bioanalyzer results)\n\nLab notebook notes about how you conducted those experiments.\n\nSpreadsheet or tabular data with the data from your experiment and whatever you were measuring for your study.",
    "crumbs": [
      "Organisation and tidy data"
    ]
  },
  {
    "objectID": "organisation-tidiness.html#structuring-data-in-spreadsheets",
    "href": "organisation-tidiness.html#structuring-data-in-spreadsheets",
    "title": "Organisation and tidy data",
    "section": "",
    "text": "Regardless of the type of data you’re collecting, there are standard ways to enter that data into the spreadsheet to make it easier to analyse later. We often enter data in a way that makes it easy for us as humans to read and work with it, because we’re human! Computers need data structured in a way that they can use it. So to use this data in a computational workflow, we need to think like computers when we use spreadsheets.\nThe cardinal rules of using spreadsheet programs for data:\n\nLeave the raw data raw - do not change it!\n\nPut each observation or sample in its own row.\n\nPut all your variables in columns - the thing that vary between samples, like ‘strain’ or ‘DNA-concentration’.\n\nHave column names be explanatory, but without spaces. Use ‘-’, ’_’ or camel case instead of a space. For instance ‘library-prep-method’ or ‘LibraryPrep’is better than ’library preparation method’ or ‘prep’, because computers interpret spaces in particular ways.\n\nDo not combine multiple pieces of information in one cell. Sometimes it just seems like one thing, but think if that’s the only way you’ll want to be able to use or sort that data. For example, instead of having a column with species and strain name (e.g. E. coli K12) you would have one column with the species name (E. coli) and another with the strain name (K12). Depending on the type of analysis you want to do, you may even separate the genus and species names into distinct columns.\n\nExport the cleaned data to a text-based format like CSV (comma-separated values) format. This ensures that anyone can use the data, and is required by most data repositories.\n\n\n\n\n\n\n\n\nEXERCISE 🧠🏋️‍♀️ – 2 mins\n\n\n\nAbove is some potential spreadsheet data generated about a sequencing experiment.\nDiscuss some of the problems with the spreadsheet data shown above. You can look at the image, or download the file to your computer via this link and open it in Excel.\n\n\n\n\n\n\nSOLUTIONS\n\n\n\n\n\nHere is a clean version of the data (download link here). Note the following changes to make the data tidy:\n\nSections reordered to be in single columns\n\nRemoved formatting/colours which won’t be interpreted by most computational tools\n\nHeader information about the reference, facility, read length etc moved to their own columns\n\nSpaces replaced in column names with _\nStandarised language for mutator and cit columns i.e., \"+\" became plus\n\nData has also been saved as a tsv file rather than excel format.\n\n\n\n\n\n\n\n\n\nBest ways to name columns:\n\nBe careful of zero values versus missing values:",
    "crumbs": [
      "Organisation and tidy data"
    ]
  },
  {
    "objectID": "outtakes-notes.html",
    "href": "outtakes-notes.html",
    "title": "Outtakes and notes",
    "section": "",
    "text": "Outtakes and notes\nNon-NGS sequencing around New Zealand:\nLincoln\n- DNA sequencing service (Sanger)\nOtago - Genetic Analysis Service (GAS) (Sanger)\ngo back over genmic data carpentry from the carpentries to see what else they go over.",
    "crumbs": [
      "_outtakes/notes"
    ]
  },
  {
    "objectID": "test-styling.html",
    "href": "test-styling.html",
    "title": "test styling",
    "section": "",
    "text": "Note\n\n\n\nstuff in note",
    "crumbs": [
      "_test styles"
    ]
  },
  {
    "objectID": "test-styling.html#header-2",
    "href": "test-styling.html#header-2",
    "title": "test styling",
    "section": "Header 2",
    "text": "Header 2\ntext\n\nHeader 3\ntext\n\nHeader 4\ntext\n\nHeader 5\ntext",
    "crumbs": [
      "_test styles"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "This is a beginner-friendly workshop, designed to get you started with the world of genomics. Whatever you’re doing—whether it’s transcriptomics, genome assembly, variant calling, metagenomics, or something else—if you will be using genomic data this workshop is for you!\n\n\n\n\nOrganisation—from messy lab books and excel spreadsheets to tidy, computer-friendly data\n\nWorking with sequencing facilities and understanding genomic data types\n\nData storage repositories, public services and facilities, and principles of FAIR and CARE\nQuality control, wrangling of raw reads and an introduction to genomic terminology\n\n\n\n\n\nNon-NGS sequencing and services (e.g., Sanger sequencing, qPCR, genotyping, probe-based applications such as microarrays and NanoString nCounter)\n\nGenomic analysis pipelines (beyond the basics of initial quality checks of raw reads)\nThe basics of cluster or HPC resourcing and specialised software (e.g., we do not cover schedulers such as SLURM, partitions/CPUs/GPUs, chosing compute allocation allowance). See our workshop on Introduction to Bash Scripting and HPC Job Scheduler for this.\nUsing shell, beyond the very basics (e.g., we do not cover writing/submitting bash scripts, modules, accessing the cluster using ssh)\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nbp\nBase pair\n\n\nHCS\nHigh Capacity Storage\n\n\nHPC\nHigh Performance Computing\n\n\nMb\nMegabase pair (1,000,000 bp)\n\n\nMB\nMegabyte\n\n\nGb\nGigabase pair (1,000,000,000 bp)\n\n\nGB\nGigabyte\n\n\nNGS\nNext-generation sequencing\n\n\nSE / PE\nSingle-end / Paired-end\n\n\n\n\n\n\nCopyright\nhttps://datacarpentry.github.io/organization-genomics/\nhttps://datacarpentry.github.io/wrangling-genomics/\nillumina lib prep 1 pic from Illumina Stranded mRNA Prep, Ligation Data Sheet. M-GL-02143 v1.0\nillumina lib prep 2 pic from llumina Stranded mRNA Prep, Ligation Protocol Document # 1000000124518 v04 pacbio smrtbell image pacific biosciences Template Preparation and Sequencing Guide P/N 000-710-821-13\n\n\n\n\nMade with ❤️ and Quarto",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#getting-started-with-genomics",
    "href": "index.html#getting-started-with-genomics",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "This is a beginner-friendly workshop, designed to get you started with the world of genomics. Whatever you’re doing—whether it’s transcriptomics, genome assembly, variant calling, metagenomics, or something else—if you will be using genomic data this workshop is for you!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#whats-covered-in-this-workshop",
    "href": "index.html#whats-covered-in-this-workshop",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "Organisation—from messy lab books and excel spreadsheets to tidy, computer-friendly data\n\nWorking with sequencing facilities and understanding genomic data types\n\nData storage repositories, public services and facilities, and principles of FAIR and CARE\nQuality control, wrangling of raw reads and an introduction to genomic terminology",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#whats-not-covered-in-this-workshop",
    "href": "index.html#whats-not-covered-in-this-workshop",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "Non-NGS sequencing and services (e.g., Sanger sequencing, qPCR, genotyping, probe-based applications such as microarrays and NanoString nCounter)\n\nGenomic analysis pipelines (beyond the basics of initial quality checks of raw reads)\nThe basics of cluster or HPC resourcing and specialised software (e.g., we do not cover schedulers such as SLURM, partitions/CPUs/GPUs, chosing compute allocation allowance). See our workshop on Introduction to Bash Scripting and HPC Job Scheduler for this.\nUsing shell, beyond the very basics (e.g., we do not cover writing/submitting bash scripts, modules, accessing the cluster using ssh)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\nbp\nBase pair\n\n\nHCS\nHigh Capacity Storage\n\n\nHPC\nHigh Performance Computing\n\n\nMb\nMegabase pair (1,000,000 bp)\n\n\nMB\nMegabyte\n\n\nGb\nGigabase pair (1,000,000,000 bp)\n\n\nGB\nGigabyte\n\n\nNGS\nNext-generation sequencing\n\n\nSE / PE\nSingle-end / Paired-end",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "Copyright\nhttps://datacarpentry.github.io/organization-genomics/\nhttps://datacarpentry.github.io/wrangling-genomics/\nillumina lib prep 1 pic from Illumina Stranded mRNA Prep, Ligation Data Sheet. M-GL-02143 v1.0\nillumina lib prep 2 pic from llumina Stranded mRNA Prep, Ligation Protocol Document # 1000000124518 v04 pacbio smrtbell image pacific biosciences Template Preparation and Sequencing Guide P/N 000-710-821-13",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "Made with ❤️ and Quarto",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html",
    "href": "data-wrangling-and-processing.html",
    "title": "Genomic data wrangling and processing",
    "section": "",
    "text": "Your data is now on a server/cloud/local computer, ready for analysis. Now what?\n\n\nBelieve it or not, computers can make mistakes! When your data files are being copied over from the sequencing facility to cloud storage, errors can occur that cause your files to become corrupted. This is not always obvious based on file size or a quick look at the contents. The first thing you’ll want to do is run a checksum, which will output a fixed-length message of 16 bytes– a unique identifier of each file. The files should also come with the checksums run by the technician at the sequencing facility. Check out the section on Data Integrity in our GA shell workshop for more information on how to run this.\n\n\n\nThe files you will get back from the sequencing facility will most likely end in .fastq.gz\nThe .gz indicates it is a compressed gzip file. For most genomic analyses, you do not need to unzip these first–this saves you storage space!\nThe fastq indicates the file is in FASTQ format. FASTQ files are a text-based format that stores the raw read sequences, along with a quality score for each base.\nThe quality score for illumina comes from the intensity of the fluorescent signal during sequencing by synthesis. For nanopore, requires an additional step to convert the electrical current change into the equivalent format? can be trickier?\n[INSERT PICTURE OF FASTQ FILES - mayeb spotty data as real example?]\n\nfastq, gz, r1 and r2 files\nquality encoding\n\nfastqc in depth (use to teach intro shell )\n\n\n\n\nintro to data types and words you’ll hear:\n\nVCF files\nBAM files\n\ngtfs\nfa (shorthand?)\n\n\n\n\n\n\nSomewhat colloquially, you will hear geneticists refer to “good” genome assemblies and “bad” genome assemblies.But how do you know if the species you are working on has a good or bad genome, if it has one at all?\nThere are a few metrics we use\nN50 busco score contig level, scaffold level, chromosome level.\nShow two exmaples screenshot from NCBI of good and bad. (and excellent?)\nThere also can be a good assembly - highly contiguos sequence (once the reads are assembled into bigger strings we call them contigs), as clsoe to actual chromosoems as possible and with it may or may not have good annotations - this is actually describing where all the genomic features are, e.g., gene start and end points, intron and exon coordinates, promoter regions, which genes are protein-coding, pseudogenes, snoRNA, tRNA, CDS, gene_id ; transcript_id ; db_xref\n\nNOT trimming - can be covered in respective data type workshop.",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#check-that-the-data-are-not-corrupt",
    "href": "data-wrangling-and-processing.html#check-that-the-data-are-not-corrupt",
    "title": "Genomic data wrangling and processing",
    "section": "",
    "text": "Believe it or not, computers can make mistakes! When your data files are being copied over from the sequencing facility to cloud storage, errors can occur that cause your files to become corrupted. This is not always obvious based on file size or a quick look at the contents. The first thing you’ll want to do is run a checksum, which will output a fixed-length message of 16 bytes– a unique identifier of each file. The files should also come with the checksums run by the technician at the sequencing facility. Check out the section on Data Integrity in our GA shell workshop for more information on how to run this.",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#genomic-file-types",
    "href": "data-wrangling-and-processing.html#genomic-file-types",
    "title": "Genomic data wrangling and processing",
    "section": "",
    "text": "The files you will get back from the sequencing facility will most likely end in .fastq.gz\nThe .gz indicates it is a compressed gzip file. For most genomic analyses, you do not need to unzip these first–this saves you storage space!\nThe fastq indicates the file is in FASTQ format. FASTQ files are a text-based format that stores the raw read sequences, along with a quality score for each base.\nThe quality score for illumina comes from the intensity of the fluorescent signal during sequencing by synthesis. For nanopore, requires an additional step to convert the electrical current change into the equivalent format? can be trickier?\n[INSERT PICTURE OF FASTQ FILES - mayeb spotty data as real example?]\n\nfastq, gz, r1 and r2 files\nquality encoding\n\nfastqc in depth (use to teach intro shell )\n\n\n\n\nintro to data types and words you’ll hear:\n\nVCF files\nBAM files\n\ngtfs\nfa (shorthand?)",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#good-vs-bad-genomes",
    "href": "data-wrangling-and-processing.html#good-vs-bad-genomes",
    "title": "Genomic data wrangling and processing",
    "section": "",
    "text": "Somewhat colloquially, you will hear geneticists refer to “good” genome assemblies and “bad” genome assemblies.But how do you know if the species you are working on has a good or bad genome, if it has one at all?\nThere are a few metrics we use\nN50 busco score contig level, scaffold level, chromosome level.\nShow two exmaples screenshot from NCBI of good and bad. (and excellent?)\nThere also can be a good assembly - highly contiguos sequence (once the reads are assembled into bigger strings we call them contigs), as clsoe to actual chromosoems as possible and with it may or may not have good annotations - this is actually describing where all the genomic features are, e.g., gene start and end points, intron and exon coordinates, promoter regions, which genes are protein-coding, pseudogenes, snoRNA, tRNA, CDS, gene_id ; transcript_id ; db_xref\n\nNOT trimming - can be covered in respective data type workshop.",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  }
]