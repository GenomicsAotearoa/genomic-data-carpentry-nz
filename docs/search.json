[
  {
    "objectID": "data-wrangling-and-processing.html",
    "href": "data-wrangling-and-processing.html",
    "title": "Genomic data wrangling and processing",
    "section": "",
    "text": "WORK IN PROGRESS",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#command-line-interface-cli-vs-graphical-user-interface-gui",
    "href": "data-wrangling-and-processing.html#command-line-interface-cli-vs-graphical-user-interface-gui",
    "title": "Genomic data wrangling and processing",
    "section": "Command-line interface (CLI) vs graphical user interface (GUI)",
    "text": "Command-line interface (CLI) vs graphical user interface (GUI)\nCommand line interface (CLI) and graphic user interface (GUI) are different ways of interacting with a computer‚Äôs operating system. They have different pros and cons. Most people are familiar with the GUI (i.e., point and click) as it is the default interface for most software, particularly on Windows and Mac OS. When using the GUI, you see and interact with visual representations of files, folders, applications, and most other functions of your computer. When using the CLI, you work largely with text representations of software, files, folders, input and output. The shell is a program that allows you to control your computer by typing instructions on the CLI with a keyboard.\nThere are several reasons to learn how to use the CLI:\n\nFor most bioinformatics tools, there are no graphical interfaces. If you want to work in metagenomics or genomics, you‚Äôre going to need to use the CLI/ shell.\n\nThe shell gives you power and allows you to work more efficiently. Tasks that are repetitive (e.g.¬†renaming hundreds of files) can be automated. Tasks that are tedious (e.g.¬†testing a range of input parameters) can be simplified.\n\nTo use remote computers or cloud computing, you often need to use the shell.\n\nHere we will not show you how to use the CLI; we recommend checking out our workshops on Introduction to Shell and Intermediate Shell.\n\nNote: you‚Äôll hear the terms ‚Äòshell‚Äô, ‚Äòbash‚Äô, ‚Äòunix‚Äô, ‚Äòterminal‚Äô and ‚Äòcommand-line‚Äô used almost interchangeably. For beginners, you can think of them as essentially all the same thing! As you gain more computational skills, you can dig into the differences.",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#understanding-genomic-file-types",
    "href": "data-wrangling-and-processing.html#understanding-genomic-file-types",
    "title": "Genomic data wrangling and processing",
    "section": "Understanding genomic file types",
    "text": "Understanding genomic file types\nThe files you will get back from the sequencing facility will most likely end in .fastq.gz\nThe .gz indicates it is a compressed gzip file. For most genomic analyses, you do not need to unzip these first‚Äìthis saves you storage space, as unzipped files can be double or triple the zipped size! Your zipped sequence read files will likely be around 1-10 GB per file.\nThe fastq indicates the file is in FASTQ format. FASTQ files are a text-based format that stores the raw read sequences, along with a quality score for each base.\nThe first 4 lines of a fastq file describe one sequence read:\n\nThe first line is the header information, beginning always with an @. This is the instrument-specific, run identifier and read identifiying information.\n\nV350304715 = instrument-specific\n\nL3 = lane 3 on the flow cell. Can be used for troubleshooting if something globally went wrong with sequencing.\n\nC001 = cluster identifier (can be used for troubleshooting, same as above).\n\nR001 = Read 1. All the reads in this file are read 1. If you have paired-end data, you‚Äôll have a corresponding read 2 file.\n\n00020454 = unique identifier for that read.\n/1 = indicates this is a pair member. Only paired-end data have the /1 or /2 additional identifier.\n\nEach instrument and chemistry will have slightly different header information. The main thing to look out for are indications of the data being read 1 or read 2 (indicates antisense or sense strand data, for protocols that can differentiate strandedness) and whether it is part of a mate pair (indicated by the /1 or /2)\nThe second line is the actual sequence. Count the number of bases ‚Äì which read length was chosen for this dataset? (It‚Äôs ok you don‚Äôt have to count - it‚Äôs 100)\nThe third line is just a ‚Äú+‚Äù. This can be optionally followed by the same sequence identifier (and any description) again.\nThe fourth line is the quality score. Each base in line 2 has a corresponding quality score, indicated by an ASCII character in line 4.\nFrom left to right, these are the quality scores in increasing order of quality:\n!\"#$%&'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\nSo, if a single base has a quality score of !, which corresponds to a Q-score of 0, this indicates exceptionally poor quality. The sequencing machine was not able to resolve which base it should be. Often these will be shown as N‚Äôs.\nConversely, if a single base has a quality score of I, which corresponds to a Q-score of 40, this indicates very high quality. The sequencing machine was ‚Äúvery confident‚Äù that the base called is correct (Q40 indicates an error probabilty of 0.0001, i.e., a 1 in 10,000 chance of the base being called wrong, on average. I is near the upper end of typically achieved quality values, but higher is possible.\nFor more on quality scores, you can also read this Illumina guide here. Note there are different types of encoding, but you will most frequently encounter Phred+33.\nWe can assess the quality score across all reads using software called FastQC, and trim down reads to remove poorer quality bases below a set threshold (generally with newer sequence data, you can use Q30 or Q40 as a cut-off). We won‚Äôt cover trimming here, see our workshop on RNA-seq Data Analysis for a step-by-step guide.\n\nONT\nNanopore sequencing works quite differently to Illumina and PacBio. The default read output is POD5 and this is converted to unaligned bam.\n\n\n\n\n\n\nFILL OUT\n\n\n\nHow are most seq facilities sending these reads to people?\nONT is pod5 -&gt; bam (unaligned)\n\n\n\n\nOther genomic file types\nWe‚Äôve talked about fastq and gz files types, but there are some other common genomic file types you should know about.\nYou should also be aware that the extension on the file type is largely there just so you as a human know what kind of file it is. When using the CLI, the computer does not care what the extension says.\n\n\n\nExtension\nName\nDescription\n\n\n\n\n.fasta or .fa, .fna, .faa\nFASTA, FASTA nucleic acid, FASTA amino acid\nContains sequence data. Each sequence starts with a single header line starting with a &gt;, followed by the nucleotide (or amino acid in case of .faa) sequence on the second line. No quality encoding included.\n\n\n.bam .sam\nBinary/Sequence Alignment Map\nContains alignment information of your reads/sequences that are mapped against a reference sequence (e.g., genome). BAM are the compressed, computer-readable only version; SAM are human-readable but larger files. Somewhat interchangeable file types. Note for ONT files, the bam file is often used in an un-aligned format. You may also encounter .cram files, which are Compressed Reference-oriented Alignment Map designed to be an efficient reference-based alternative to bam/sam\n\n\n.vcf\nVariant Call Format\nContains sequence variants called relative to a reference genome. Split into two sections; the header information and the records. See our Intro R workshop, section on VCF files.\n\n\n.gtf or .gff\nGeneral Transfer Format or General Feature Format\nContains genome annotation data.\n\n\n\n\n\nReal examples:\n\n\n\n\n\n\nFASTA\n\n\n\n\n\nFASTA file containing cds data from a genome, opened in a text editor. Each sequence is preceded by one header line, beginning with an ‚Äò&gt;‚Äô symbol. \n\n\n\n\n\n\n\n\nIf we did not already know this fasta file contains cds data, what gives it away?\n\n\n\n\n\nThe sequence starts with a start codon ‚ÄòATG‚Äô and ends with a stop codon ‚ÄòTAA‚Äô. Also, ‚Äúcds‚Äù can be seen in the header lines as part of the sequence accession identification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBAM/SAM\n\n\n\n\n\nSAM file, opened in a text editor. Note all lines in the header section starts with an ‚Äò@‚Äô symbol, and the number of lines in the header section is variable. There are 11 mandatory fields in the alignment section (not all visible here as they run off the window to the right). \n\n\n\n\n\n\n\n\n\nVCF\n\n\n\n\n\nVCF file opened in excel. Instead of the header sections seen in the other file types, here each column has a name. (not all columns visible as they run off the window to the right) \n\n\n\n\n\n\n\n\n\nGTF\n\n\n\n\n\nGTF file opened in a text editor. Note all lines in the header section start with a ‚Äò#‚Äô symbol. Generally each GTF file you encounter will all have 4 or 5 lines in the header section, but the alignment section will vary in size based on number of annotations.",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#fastqc---quality-scoring-of-raw-reads",
    "href": "data-wrangling-and-processing.html#fastqc---quality-scoring-of-raw-reads",
    "title": "Genomic data wrangling and processing",
    "section": "FASTQC - quality scoring of raw reads",
    "text": "FASTQC - quality scoring of raw reads\nAll fastq files, regardless of which kind of sequencing was performed, should be interrogated using a program called FastQC. Most HPC services (e.g., REANNZ) come with this program (and many other commonly used programs) pre-installed, so you likely will not need to install it anywhere.\nTwo of our genomic analysis workshops (RNA-Seq Data Analysis and Microbial genome assembly) teach you how to run fastqc on your files. Here, we will give you an introduction to what it is and how to interpret the results.\nFastQC will give you a good overview of the quality of the reads (i.e., how confidently each base was called by the sequencing platform) and a few other statistics about the reads. Each statistic will be assigned a PASS, WARNING or FAIL:\n PASS ¬†¬†¬†  WARNING ¬†¬†¬†  FAIL\nThe following examples come from the ‚ÄòGood Illumina Data‚Äô and ‚ÄòBad Illumina Data‚Äô reports from the Babraham Bioinformatics website.\n\nCaveat: FastQC can be run on any .fastq data, but the pass, warning, fail metrics are based on Illumina data. ONT data in particular may receive fails, even when it is perfectly usable.\n\n\nEXAMPLES: Good vs bad sequence data\nBelow are the summarised pass, warning, fail statistics for the good (left) and bad (right) data. \nHere we will choose a few of these statistics as examples to delve into:\n\nPer base sequence quality\nThe per base sequence quality is the first plot you can use to assess the quality. You can picture the reads as being all stacked on top of each other, and the average quality at each position (x-axis) shown as a boxplot (i.e., position 1 shows a boxplot of the quality scores of base pair #1 in all reads, position 2 shows a boxplot for base pair #2, and so on).\nThe quality score is shown on the y-axis. A score of below Q20 (error probabilty = 0.01, or 1 in 100 bases are incorrect on average) is shown as red - boxplots that extend in to this section indicate positions in the read that are poor quality.\nIn general, when you are trimming your reads you should pick a quality cut off of at least Q30 (error probabilty = 0.001; 1 in 1000), and some researchers even use Q40 (error probabilty = 0.0001; 1 in 10,000) for newer data. If you are working with older data (10+ years old, sequenced before ~2014), you may need to pick Q20 as a cut-off. These plots help you decide what is best for your data.\nAfter trimming reads, you should run FastQC again to compare how the average quality has changed and see how many reads are retained after trimming.\nIn this ‚Äògood data‚Äô below, all boxplots sit easily in the green range. These reads are excellent quality (i.e., the sequencing itself worked very well).\nGood data: Per base sequence quality \nIn this ‚ÄòBad data‚Äô below, we can see most reads are dropping in to the Q20 and below range from about base position 24 onwards to position 40. This is a typical pattern, where sequence quality tends to drop off around the 3‚Äô end for Illumina reads (even for good data ‚Äì and typically for paired end data, the 3‚Äô end of read 2 tends to be slightly poorer quality than read 1).\nBad data: Per base sequence quality \n\n\nPer sequence quality scores\nThe per sequence quality scores plots the Q-score per sequence, rather than per base position as we saw above. The average Q-score for the whole read is taken, then plotted to show how many reads have that average score. Below we can see that for the good data and bad data, almost all reads sit above Q30.\nGood data: Per sequence quality scores \nFor the bad data, we can see that the larger peak of reads sits at a slightly lower (and wider) Q score, and there is a second peak at a very poor score of around Q17. These data show us that there may be a large proportion of reads with a high Q-score retained after trimming (good news!). The data has been given a ‚Äòpass‚Äô by FastQC, indicating it has not dropped below the threshold for this category to be given a warning or fail.\nBad data: Per sequence quality scores \n\n\nOverrepresented sequences\nMultiple sources can cause overrepresented sequences in your dataset. It can be a true biological or experimental effect, for example if you expect a few genes to be exceptionally highly expressed or if you have done targeted amplicon sequencing. However, it often reflects an issue that may have happened during extraction or library prep. Contaminating rRNA is a common culprit in RNAseq, as rRNA exists at a much higher concentration than other mRNAs in the cell. You may have to make a judgement call whether your data is still useable if it is highly contaminated. While contaminating reads can be filtered out, you‚Äôll have less useable reads. PCR overamplification during the library prep can also cause overrepresentation - in which case it may be possible to repeat part of the library prep with less PCR cycles.\nFor our good data, no overrepresented sequences were detected:\n\nHere is a snippet of our bad data, showing a ‚Äòwarning‚Äô with multiple overrepresented sequences detected, and the count and percentage of the total reads is also shown:\n\nFastQC looks for hits that are matches in a database of common contaminants. No hit was detected for any of these sequences, but it is worth BLASTing some of the highest count sequences to find the culprit.\n\n\n\n\n\n\nEXERCISE üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (1 min)\n\n\n\nWhat percentage of reads would need to be overrepresented for this category to be given a fail?\nUse the FastQC help guide here.\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\nIf any sequence is found to represent more than 1% of the total it will get a fail.",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#good-genome-bad-genome",
    "href": "data-wrangling-and-processing.html#good-genome-bad-genome",
    "title": "Genomic data wrangling and processing",
    "section": "Good genome, bad genome?",
    "text": "Good genome, bad genome?\n\n\n\n\n\n\nTHOUGHTS\n\n\n\nThis section sits out a little odd here, but I think its needed in the workshop. To me its sort of ‚Äòwrangling your brain‚Äô around genomes\n\n\nYou will often hear geneticists refer (somewhat colloquially) to ‚Äúgood‚Äù and ‚Äúbad‚Äù genome assemblies. But what does this actually mean? And how can you tell whether a species you are working on has a good genome assembly?\nIn practice, we assess genome assemblies using a few metrics that capture the three C‚Äôs: contiguity, correctness, and completeness.\nIn our Long read genome assembly workshop, we go into further detail on how to assess these metrics. For now, we‚Äôll focus on an overview to get you started:\nCommon genome assembly metrics\n\nContiguity describes how complete or fragmented the genome assembly is. Because we can‚Äôt sequence an entire chromosome in one long piece, the genome is first broken into many smaller fragments during library prep and sequencing. The more pieces we can put together (i.e., assemble) to make longer strings of sequence, the more contiguous our assembly is. Contiguity is commonly summarised using contig N50 and scaffold N50. The N50 is a length statistic and you can think of it as similar to the median. The N50 value is the length of the first contig that is equal to or greater than half of the assembly sum. In simple terms, the larger than better. A larger N50 means that more of the genome is contained in longer sequences. In this era of long-read genome assemblies, a contig N50 over 1 Mb is generally considered good - see Excerpt from PacBio article.\nCompleteness can be measured in multiple ways, but it is often measured using a BUSCO (Benchmarking Universal Single-Copy Orthologs) score. This measures completeness using sets of conserved, single-copy genes expected to be present in most species within a specified lineage, which are reported as percentages of complete, duplicated, fragmented, and missing genes, to a total of 100%. Essentially, we check to see if a bunch of genes we expect to see in every species are present in our assembly.\nCorrectness describes the base pair and structural accuracy of the genome. It can be measured by comparing your assembly to a reference genome. There are a few limitations to this approach, namely that if a good reference genome for your species does not exist you will have to use a different approach (for which there are solutions - see our long read genome assembly workshop link above).\n\nGenome assemblies can be described more qualitatively as (in order of least to most complete):\n\nContig-level ‚Äì sequences are assembled but not ordered or oriented. Often can be thousands to hundreds of thousands of contigs.\n\nScaffold-level ‚Äì contigs linked together, often with gaps. None are placed (i.e., chromosome known) or localised (i.e., orientation on chromosome known).\n\nChromosome-level ‚Äì one or more chromosomes complete, but some unplaced scaffolds may be present.\n\nComplete genome ‚Äì all chromosomes are resolved with no gaps. Not necessarily telomere-to-telomere (T2T). Very rare to have a complete genome assembly.\n\nSee NCBI Glossary for Assembly level.\n\n\nWhat do we mean by a ‚Äúgood‚Äù genome?\nA good genome assembly is typically one that is:\n\nHighly contiguous\nReads have been assembled into long sequences called contigs, ideally approaching full chromosome lengths.\nBiologically complete\nMost expected genes are present (high BUSCO completeness).\nStructurally accurate\nLarge-scale errors such as mis-joins or collapsed repeats are minimised.\n\n\nNote: often the word ‚Äòcontig‚Äô is used to refer to any length of assembled (i.e., contiguous) sequence, regardless of if those sequences could technically be long enough to be called scaffolds or chromosomes.\n\n\n\nAssembly quality vs annotation quality\nLastly, it is important to distinguish between:\nGenome assembly i.e., the reconstructed DNA sequence itself, and the Genome annotation, which is the identification and labelling of genomic features, including gene structures (exons, introns, and CDS), non-coding RNAs (e.g., rRNA, tRNA and lncRNA), and associated identifiers such as gene_id, transcript_id, and database cross-references.\nA genome can be well assembled but poorly annotated, or vice versa.\n\nWatch out! It can be tempting based on the BUSCO score to conclude that the ‚Äúannotations‚Äù for a genome are quite good (or bad). The BUSCO score IS NOT the genome annotation; it is an independent analysis of a set of known genes. The annotation file itself (scroll up to section Understanding genomic file types for description of annotation files such as GTF files) is not being assessed when we look at the BUSCO score.\n\n\n\n\nEXERCISES üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (8 mins):\nHow would you describe these genomes?\nLook at the assembly stats and evaluate how ‚Äògood‚Äô you think each assembly is.\nNote: RefSeq is the curated database. GenBank is the non-curated, anyone-can-submit-to database. They can be different, but not always. Use the RefSeq stats to make your decision.\n\n\n\n\n\n\nEXERCISE #1 üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (4 mins) - Human Genome Assembly\n\n\n\nLink here to Human Genome assembly GRCh38.p14\n\n\nJot down your thoughts before looking at the solution below.\nHere are some points to help you:\n\nWhat is the assembly level and where does that fall on our scale level of contig -&gt; scaffold -&gt; chromosome -&gt; complete?\n\nHow many chromosomes, if any, are resolved? (Hint: if there are any resolved it is a good assembly!)\nWhat is the scaffold N50? Is it above 1Mb? (typically considered a fairly good assembly if &gt;1 Mb).\nWhat is the BUSCO score? Is it approaching 100% complete?\n\n\n\n\n\n\n\nHuman genome solution\n\n\n\n\n\n\nThe assembly is at Chromosome level - indicates very high quality.\nThe BUSCO score is 98.9% - very close to complete.\n\nThe number of scaffolds (470) and contigs (996) are very low at less than a thousand each, indicating the majority of sequences are assembled into the chromosomes. Further, the scaffold and contig N50s are very high, well over 1 Mb, indicating further that the majority of these sequences are very long contiguous sequences.\n\nConclusion: this is a very good genome assembly.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE #2 üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (4 mins) - Sea Anemone Genome Assembly\n\n\n\nLink here to Nematostella vectensis (sea anemone) Genome assembly ASM20922v1\n\n\nJot down your thoughts before looking at the solution below.\nHere are some points to help you:\n\nWhat is the assembly level and where does that fall on our scale level of contig -&gt; scaffold -&gt; chromosome -&gt; complete?\n\nHow many chromosomes, if any, are resolved? (Hint: if there are any resolved it is a good assembly!)\nWhat is the scaffold N50? Is it above 1Mb? (typically considered a fairly good assembly if &gt;1 Mb).\nWhat is the BUSCO score? Is it approaching 100% complete?\n\n\n\n\n\n\n\nSea anemone solution\n\n\n\n\n\n\nThe assembly is at scaffold level; no chromosomes are resolved - this is a ‚Äúmedium‚Äù quality genome.\nThe scaffold N50 is &lt;1 Mb; at 472.6 kb - indicating again the quality is ‚Äúmedium‚Äù or ‚Äúaverage‚Äù.\nThe BUSCO score is pretty good at 97.7% single copy orthologs resolved - this indicates that the scaffolds resolved are likely long enough and correct enough to resolve the majority of genes. Functionally, this is quite a useable genome at a gene level.\n\nConclusion: this is a medium or average assembly. Larger scale structural variants may be difficult or erroneous to resolve, but gene level comparisons and analyses could be achieved fairly accurately.\n\n\n\n\n\n\nExtra for experts\n\n\n\nHere is the original paper for the curious: Putnam et al.¬†2007.\nThe authors claim the genome reveals ‚Äúancestral eumetazoan genomic organization‚Äù. How did they assess this and what conclusion did they make about the eumetazoan ancestor resemblance to modern vertebrates, sea anemones and protosomes? Do you think they had a ‚Äògood‚Äô enough genome to assess this?\n\n\n\n\n\n\nExtra for experts solution\n\n\n\nNEED TO FACT CHECK THIS, COLLAPSE WHEN COMPLETE\nThe authors used synteny analysis. They found many major genes, such as Hox genes, retained the same intron/exon structure in sea anemones as vertebrates, but this was not retained in model protosome species. Based on this they concluded that sea anemone genomes more closely resemble vertebrates than protosome species (fly, nematode, annelid) do. While the genome assembly itself is somewhat fragmented, the BUSCO score was quite good, indicating that any conclusions the authors made comparing genomes and a gene stucture level are likely quite robust, but it would not be feasible with this assembly to make any conclusions based on larger genomic structure (i.e., at closer to chromosome level).",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-wrangling-and-processing.html#summary",
    "href": "data-wrangling-and-processing.html#summary",
    "title": "Genomic data wrangling and processing",
    "section": "Summary",
    "text": "Summary\nclose off with a quick summary of what the workshop has covered,\na bullet point list of other GA workshops (with links) and a one sentence description of what is covered.",
    "crumbs": [
      "Genomic data wrangling and processing"
    ]
  },
  {
    "objectID": "data-storage-and-management.html",
    "href": "data-storage-and-management.html",
    "title": "Genomic data storage and management",
    "section": "",
    "text": "WORK IN PROGRESS",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#genomic-data-files",
    "href": "data-storage-and-management.html#genomic-data-files",
    "title": "Genomic data storage and management",
    "section": "Genomic data files",
    "text": "Genomic data files\nGenomic data files are often huge, and for one project you will often have several hundred gigabytes (GB) to sometimes terabytes (TB) of data (more on the actual genomic file types in the next section Data wrangling and processing). You most likely won‚Äôt be able to store and analyse these on your local computer, or transfer the files in the ‚Äòtraditional‚Äô way using portable USB drives, so you‚Äôll need to make use of a high capacity storage (HCS), high performance computing (HPC), and file transfer protocols (FTP). An introduction to genomics is also an introduction to the world of high performance computing!\nCaveat: small genomes such as from bacteria often can be transferred, stored and analysed on your local computer.\nYou have a few options of how you want to store and analyse your data, which may be through HPC/HCS services offered by your University or Institute, or through the national computing and digital network REANNZ.",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#research-data-management",
    "href": "data-storage-and-management.html#research-data-management",
    "title": "Genomic data storage and management",
    "section": "Research Data Management",
    "text": "Research Data Management\nUniveristy have RDM policies\neg auckland https://research-hub.auckland.ac.nz/managing-research-data/ethics-integrity-and-compliance/research-data-management-policy-guidance",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#research-and-education-advanced-network-new-zealand-reannz",
    "href": "data-storage-and-management.html#research-and-education-advanced-network-new-zealand-reannz",
    "title": "Genomic data storage and management",
    "section": "Research and Education Advanced Network New Zealand (REANNZ)",
    "text": "Research and Education Advanced Network New Zealand (REANNZ)\nREANNZ is ‚Äú..a Crown-owned membership organisation that powers Aotearoa‚Äôs research and education network. Launched in 2007, their high-performance national digital network (or NREN) helps members collaborate and contribute to data-intensive and complex science and research initiatives ‚Äì both here in New Zealand and across the globe.‚Äù\nThey offer many products and services ‚Äì check out their website here ‚Äì but here we will focus on their HPC platform that you may chose to use to analyse your data.\nProjects on REANNZ\nTo use the HPC services, you first need to make or be listed as a member on a ‚ÄòProject‚Äô. A ‚ÄòProject‚Äô will have a code (e.g., nesi03181 or uoo00431) and will need to be linked to a funding source to charge back compute resources used. Multiple people can use one Project; this is often the preferable way to use a Project when collaborating on the same dataset or project. You‚Äôll need to discuss whether you need your own Project, or can be added to an existing Project, with whoever is supplying the funding (most likely your PI / supervisor).\nEither way, first make your own account using your institutional details by logging in here: https://my.nesi.org.nz/.\n\nFrom there, you can apply/access/view your Projects, and manage the compute resource allocation of the Project.\nOnce you have an account and a Project, you can start using the HPC by logging in to Open OnDemand here: ondemand.nesi.org.nz\n\nOnce logged in, you‚Äôll see an app-based dashboard.\nClick on ‚ÄòJupyter Lab‚Äô and then you can chose a few different settings before launching. For the most part, you will only need to change the Project Code and the number of hours you need the session open. You can leave the number of cores and memory per job as the default lowest settings.\n\nOnce the session is open, you can interact with your files through the directory structure on the left, and open either a Terminal (see our Introduction to Shell workshop for more on using Terminal) or various other scripting programs through the Launcher.\nFor each Project, you will have a nobackup and a project directory. Keep your raw files in the project directory and do your analysis in the nobackup directory. As you can guess by the name, this directory is not backed up, and is best used for working analyses. Additionally, files not modified after 90 days are auto deleted. Final analysed files and imprortant scripts should be copied into the project directory once you have completed your analysis. You may be surprised by how many intermediate files you make and how much ‚Äòtinkering‚Äô you do during your analyses‚Äìmaking use of the two directories helps keep your file system tidier.\n\nNote: It can be a good idea to back-up raw genomic files on a HCS / reannz freezer? FILL\n\nGetting your genomic files on to the REANNZ cluster\n\n\n\n\n\n\nFILL IN HERE\n\n\n\nHow do we actually get the genomic sequence files from the sequencing facility to the REANNZ cluster?\nGLOBUS\n\n\nWant more info?\n\nREANNZ provide extensive documentation on how to use their High Performance Computing (HPC) platforms.\n\n\nIn July 2025, the New Zealand eScience Infrastructure (NeSI) was integrated into REANNZ. You may have heard of NeSI before, but if not, it is good to be aware of the name, as you‚Äôll see some legacy branding in the REANNZ documentation or log-in platform (e.g., my.nesi.org.nz is still in use).",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#aoraki-at-the-university-of-otago",
    "href": "data-storage-and-management.html#aoraki-at-the-university-of-otago",
    "title": "Genomic data storage and management",
    "section": "Aoraki at the University of Otago",
    "text": "Aoraki at the University of Otago\nIf you are a University of Otago staff or student, you can use the ‚ÄúAoraki‚Äù HPC cluster. Like REANNZ, you will also connect to the cluster through an Open OnDemand app-based interactive web page.\nhttps://rtis.cspages.otago.ac.nz/research-computing/cluster/index.html",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#centre-for-eresearch-cer-at-university-of-auckland",
    "href": "data-storage-and-management.html#centre-for-eresearch-cer-at-university-of-auckland",
    "title": "Genomic data storage and management",
    "section": "Centre for eResearch (CeR) at University of Auckland",
    "text": "Centre for eResearch (CeR) at University of Auckland\nresearch compute, supports access to REANNZ HPC. https://research-hub.auckland.ac.nz/research-infrastructure/research-platforms/eresearch",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#other-universitiesinstitutesorganisations-hpchcs-options",
    "href": "data-storage-and-management.html#other-universitiesinstitutesorganisations-hpchcs-options",
    "title": "Genomic data storage and management",
    "section": "Other universities/institutes/organisations HPC/HCS options?",
    "text": "Other universities/institutes/organisations HPC/HCS options?\n\n\n\n\n\n\nFILL IN HERE\n\n\n\nwellingotn? massey? UC? PROs? probably cant cover all but need to list some of these.\n\n\n\nAotearoa Genomic Data Repository (AGDR)\nThe AGDR is enabled by MBIE funding to Genomics Aotearoa. The AGDR is accepted as a suitable repository for (non-human) genomic data by many journals.\nFrom the AGDR website:\n‚ÄúThe Aotearoa Genomic Data Repository provides secure within-nation storage, management and sharing of non-human genomic data generated from biological and environmental samples originating in Aotearoa New Zealand. This resource has been developed to follow the principles of MƒÅori Data Sovereignty, and to enable kaitiakitanga (guardianship), so that iwi, hap≈´ and whƒÅnau (tribes, kinship groups and families) can effectively exercise their responsibilities as guardians over biological entities that are taonga (precious or treasured). While the repository is designed to facilitate the sharing of data ‚Äî making it findable by researchers and interoperable with data held in other genomic repositories ‚Äî the decision-making process regarding who can access the data is entirely in the hands of those holding kaitiakitanga over each data set.‚Äù‚Äù",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "data-storage-and-management.html#public-data",
    "href": "data-storage-and-management.html#public-data",
    "title": "Genomic data storage and management",
    "section": "Public data",
    "text": "Public data\nYou are probably familiar with using the NIH NCBI database, hosted by the United States Government, for things such as BLAST searches and literature searches.\nNCBI has a repository called the Sequence Read Archive (SRA), which is where a large proportion of genomic data from all over the world is stored, as publicly available raw, high-throughput, sequencing data.\nIf your data has no ethical or data sovereignty concerns (e.g., your MƒÅori research partners have approved uploading the data), this can be a great resource for making your data freely available and easily citeable in publications.\nHere we will not go into detail on how to either download or upload data to the SRA (see SRA documentation on their website). But some points to be aware of are:\n\nA BioProject is a way to link all the sequencing and biological data together under one umbrella (as a general rule, 1 publication = 1 BioProject, but many research groups use a single BioProject for multiple publications). If you are uploading your own data, we recommend starting with creating a BioProject, which will then give you a single accession (e.g., PRJNA31257) to use for the next steps (create BioSample entries and then upload raw reads to SRA).\nA BioSample is a unique descriptor of the biological source material (e.g, all the biological metadata of your sample ‚Äì species, sex, age, location, tissue, etc.). This is often (but not strictly) set up as 1 BioSample = 1 library. Each BioSample accession will be linked to the matching sequence data in the SRA, and all housed under one BioProject accession number.\n\nEqually, if you are downloading data from the SRA, you can make use of BioProject and BioSample accessions to identify and orientate yourself to the data.\nExample BioProject:\n\n\n\n\n\n\nHuman airway smooth muscle transcriptome study\n\n\n\n\n\nYou can see there are 16 SRA experiments and 16 BioSamples ‚Äì you can guess that these probably match one to one.\nThere are also GEO datasets (Gene Expression Omnibus) associated with this study, which are repositories for functional genomic data. These are a more processed form of the data e.g., count tables and differential expression data.\n\n\n\n\n\nThis is not the only public repository of sequence reads, some other examples include: European Nucleotide Archive (ENA) and the DNA data bank of Japan DDBJ Sequence Read Archive (DRA). These data banks are linked together via the International Nucleotide Sequence Database Collaboration. NCBI also details some of the collaborative projects here (e.g., The Taxonomy Project, The Feature Table).",
    "crumbs": [
      "Genomic data storage and management"
    ]
  },
  {
    "objectID": "organisation-tidiness.html",
    "href": "organisation-tidiness.html",
    "title": "Organisation and tidy data",
    "section": "",
    "text": "WORK IN PROGRESS",
    "crumbs": [
      "Organisation and tidy data"
    ]
  },
  {
    "objectID": "organisation-tidiness.html#metadata",
    "href": "organisation-tidiness.html#metadata",
    "title": "Organisation and tidy data",
    "section": "Metadata",
    "text": "Metadata\nWhen we think about the data for a sequencing project, we often start by thinking about the sequencing data that we get back from the sequencing centre. However, equally or more important is the data you‚Äôve generated about the sequences before it ever goes to the sequencing centre. This is the data about the data, often called the metadata. Without the information about what you sequenced, the sequence data itself is useless.\n\n\n\n\n\n\nEXERCISE üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (3 mins)\n\n\n\nWhat kind of data and information have you generated (or think you will generate) about your samples that can be considered metadata? How do you store this information?\nDiscuss with the group. When you are ready, click on ‚ÄòSolutions‚Äô for a few examples of what these metadata could be.\n\n\n\n\n\n\nSOLUTIONS\n\n\n\n\n\n\nExperimental conditions describing the collection of biological samples, such as: temperature, weight, length, treatment, photos of the specimen, GPS coordinates, specimen accession numbers.\n\nLaboratory/technical data, such as: protocol or kit used to extract DNA/RNA, specific processing equipment, concentrations or quality indicators (e.g., Nanodrop, Qubit, or Bioanalyzer results)\n\nLab notebook notes about how you conducted those experiments.\n\nSpreadsheet or tabular data with the data from your experiment and whatever you were measuring for your study.",
    "crumbs": [
      "Organisation and tidy data"
    ]
  },
  {
    "objectID": "organisation-tidiness.html#structuring-data-in-spreadsheets",
    "href": "organisation-tidiness.html#structuring-data-in-spreadsheets",
    "title": "Organisation and tidy data",
    "section": "Structuring data in spreadsheets",
    "text": "Structuring data in spreadsheets\nRegardless of the type of data you‚Äôre collecting, there are standard ways to enter that data into the spreadsheet to make it easier to analyse later. We often enter data in a way that makes it easy for us as humans to read and work with it, because we‚Äôre human! Computers need data structured in a way that they can use it. So to use this data in a computational workflow, we need to think like computers when we use spreadsheets.\nThe cardinal rules of using spreadsheet programs for data:\n\nLeave the raw data raw - do not change it!\n\nPut each observation or sample in its own row.\n\nPut all your variables in columns - the thing that vary between samples, like ‚Äòstrain‚Äô or ‚ÄòDNA-concentration‚Äô.\n\nHave column names be explanatory, but without spaces. Use ‚Äò-‚Äô, ‚Äô_‚Äô or camel case instead of a space. For instance ‚Äòlibrary-prep-method‚Äô or ‚ÄòLibraryPrep‚Äô are better than ‚Äôlibrary preparation method‚Äô (because computers interpret spaces in particular ways) and are also better than ‚Äòprep‚Äô (more informative).\n\nDo not combine multiple pieces of information in one cell. Sometimes it just seems like one thing, but think if that‚Äôs the only way you‚Äôll want to be able to use or sort that data. For example, instead of having a column with species and strain name (e.g., E. coli K12) you would have one column with the species name (E. coli) and another with the strain name (K12). Depending on the type of analysis you want to do, you may even separate the genus and species names into distinct columns.\n\nExport the cleaned data to a text-based format like CSV (comma-separated values) format. This ensures that anyone can use the data, and is required by most data repositories.\n\n\n\n\n\n\n\nEXERCISE üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (3 mins)\n\n\n\nBelow is some potential spreadsheet data generated about a sequencing experiment.\n\nDiscuss some of the problems with the spreadsheet data shown above. You can look at the image, or download the file to your computer via this link and open it in Excel.\nWhen you are ready, click on ‚ÄòSolutions‚Äô to see a clean version of the data.\n\n\n\n\n\n\nSOLUTIONS\n\n\n\n\n\nHere is a clean version of the data (download link here). Note the following changes to make the data tidy:\n\nSections reordered to be in single columns\n\nRemoved formatting/colours which won‚Äôt be interpreted by most computational tools\n\nHeader information about the reference, facility, read length etc moved to their own columns\n\nSpaces replaced in column names with _\nStandarised language for mutator and cit columns i.e., \"+\" became plus\n\nData has also been saved as a tsv file rather than excel format.\n\n\n\n\n\n\n\n\nTidy data principles\nThe simplest principle of Tidy data is that we have one row in our spreadsheet for each observation or sample, and one column for every variable that we measure or report on. As simple as this sounds, it‚Äôs very easily violated. Most data scientists agree that a significant amount of their time is spent tidying data for analysis.\nThe R package Tidyverse was specifically developed to deal with data cleaning and manipulation for analysis. Sidenote: a Genomics Aotearoa workshop on data manipulation with tidyverse tools is in development!\n\n\n\n\n\n\nEXERCISE üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (2 min)\n\n\n\nHave a look at these column names and the differences between the good names and names to avoid:\n\nWhat do you notice? Jot down your thoughts before looking at the ‚ÄòSolutions‚Äô below:\n\n\n\n\n\n\nSOLUTIONS\n\n\n\n\n\n\nGood column names avoid using spaces and any special characters beyond underscores (_)\nCamel case is often a good alternative naming convention\n\nNote hyphens (-) are also avoided as they can cause headaches with R.\n\nR also does not like column names that begin with a number (e.g., Observation_01 is fine, 01_Observation is not)\n\n\n\n\n\n\nThere are lots of different conventions people use to represent missing values. You should especially avoid representing a missing value as a 0 (zero), as zero implies that a measurement was taken and the result was zero.\nThe best option is to leave missing values as blank or designate them as an NA in your spreadsheet.\nSee below a few common values used for indicating a missing value:",
    "crumbs": [
      "Organisation and tidy data"
    ]
  },
  {
    "objectID": "outtakes-notes.html",
    "href": "outtakes-notes.html",
    "title": "Outtakes and notes",
    "section": "",
    "text": "WORK IN PROGRESS\n\n\n\n\n\n\nOuttakes and notes\n\nchloe notes\ngo back over genmic data carpentry from the carpentries to see what else they go over.\n\ngenome seq - combining platforms to polish correct reads\n\n\n\nRakeiora Genomics platform\nThe Rakeiora Genomics Platform is designed to enable and test pilot precision medicine research, linking genomics and health.\nThey have chosen a walled garden approach to this infrastructure.\n‚ÄúThis means that all genomic data and health data, and its analysis, are undertaken only within this computational environment ‚Äì the data never leave and are certainly never downloaded to a researcher‚Äôs own computer or a hospital clinical laboratory computer‚Ä¶\n‚Ä¶Its modular nature will allow linkage (with consent) to other health data systems currently under development in Aotearoa New Zealand. This approach in particular addresses transparency of use and control over narratives. Lack of these features in other precision medicine platforms worldwide has led to significant problems for patients and research participants, including indigenous communities; learning from these negative experiences is key to avoid repeating them.‚Äù ‚Äì From the Genomics Aotearoa website.\n\n\nFirst, check that the data are not corrupt\nBelieve it or not, computers can make mistakes! When your data files are being copied over from the sequencing facility to cloud storage, errors can occur that cause your files to become corrupted. This is not always obvious based on file size or a quick look at the contents. The first thing you‚Äôll want to do is run a checksum, which will output a fixed-length message of 16 bytes‚Äì a unique identifier of each file. The files should also come with the checksums run by the technician at the sequencing facility. If the files have been corrupted by even one character, the 16 byte unique identifier will be completely different, allowing you to easily and quickly verify the integrity of your files. Check out the section on Data Integrity in our GA shell workshop for more information on how to run this."
  },
  {
    "objectID": "preparing-for-sequencing.html",
    "href": "preparing-for-sequencing.html",
    "title": "Preparing for sequencing",
    "section": "",
    "text": "WORK IN PROGRESS",
    "crumbs": [
      "Preparing for sequencing"
    ]
  },
  {
    "objectID": "preparing-for-sequencing.html#quality-control-of-dnarna",
    "href": "preparing-for-sequencing.html#quality-control-of-dnarna",
    "title": "Preparing for sequencing",
    "section": "Quality control of DNA/RNA",
    "text": "Quality control of DNA/RNA\nYour DNA or RNA samples will need to pass certain quality cut-offs for sequencing, which the sequencing facility will ask for and will have thresholds they require upon submission (e.g., see the Otago Genomics Facility requirements here). After you have done your extractions in the lab, you should perform QC with:\n\nAgarose gel electrophoresis. For RNA, this will give you a good indication of the integrity of your RNA sample; generally you will expect to see two clear bands for the 28S and 18S rRNA subunits. For DNA samples, in particular for long-read sequencing, you want to see a high molecular weight band with minimal smearing (which would indicate degradation). Getting the high-molecular-weight gDNA (i.e., long gDNA fragments) is critical to obtaining high-quality data for WGS using ONT/PacBio.\n\n\n\n\n\n\n\nAgarose gel electrophoresis logistics:\n\n\n\n\n\nMaking and performing agarose gel electrophoresis is a core molecular biology skill that we expect almost all readers will have learnt, so we will not go into further details here. Note for RNA samples, different species can have different rRNA banding patterns that may not look like a ‚Äòclassic‚Äô 28S/18S pattern (for eukaryotes), and different tissue types can have different levels of small RNAs, which can appear like degraded RNA on a gel. You may need to do some literature searching if you have odd results. For high molecular weight DNA (i.e., for genome sequencing), you may need to run a special gel e.g., a pulsed-field gel electrophoresis or a capillary-based system such as Agilent Femto Pulse System. See also assessing molecular weight in Nanopore documentation.\n\n\n\n\nA microvolume spectrophotometer (e.g., NanoDrop, DeNovix), which will give you a good indication of the purity/contamination of your sample through 260/280 and 260/230 ratios. This method is not very reliable for determining concentration of your nucleic acid.\n\n\n\n\n\n\n\nNanoDrop logistics:\n\n\n\n\n\nThere is usually a NanoDrop or DeNovix in most labs as standard benchtop equipment (NanoDrops are more common and most people refer to this QC method simply as ‚Äònanodrop‚Äô). It is free to run (no additional reagents required beyond UltraPure water/ddH2O) and only requires 1-2 ŒºL of your sample. Results appear on the screen within seconds. Gently clean with UltraPure water and a kimwipe between samples; do not use ethanol on platform. While the nanodrop is not a great option for definitive quantification of your sample concentration, it can be a good starting indication as it can measure a broader concentration range than the Qubit. It will almost always overestimate the concentration. If your Qubit to nanodrop concentration ratio is more than 50% different (e.g., nanodrop says 100 ng/ŒºL, Qubit says &lt;50ng/ŒºL), this could indicate substantial contamination of the undesired nucleic acid or other contaminant and you may need to re-purify / clean and concentrate samples.\n\n\n\n\nA fluorescent-based quantification method (e.g., Qubit), which will give you a very accurate indication of concentration of your nucleic acid (but not of any contaminants).\n\n\n\n\n\n\n\nQubit logistics:\n\n\n\n\n\nMany labs have a Qubit (e.g., see Qubit 4 model; other models exist), but they are less common than microvolume spectrophotometers. You may need to ask around your department/faculty to see where one is and who owns it. There are a few ongoing costs associated with using the Qubit (standards, buffers, dyes and tubes are required consumables), so if you are borrowing this from another group you will need to work out how the costs should be covered. The estimated cost is around $1 per sample (adds up fast when you have many samples + technical replication!). You will also have to determine which kit you need, often either the dsDNA HS (high sensitivity) assay kit or the RNA HS kit; there are also broad range kits, see assay types here. The Qubit generally requires 1 ŒºL of your sample to run. Samples for the Qubit take a few mins to prep, then results are read within seconds and displayed on the screen. DNA/RNA concentrations can both be measured very accurately, but not simultaneously. The Qubit is very sensitive, but in a more narrow concentration range to the NanoDrop. You may need to use the NanoDrop concentration as a general indication for diluting your samples to the concentration range of the Qubit kit you are using, so read the kit information to determine which one you need.\n\n\n\n\nA fragment analysis instrument (e.g., Agilent 2100 Bioanalyzer, Agilent 5300 Fragment Analyzer or Agilent TapeStation), which you can think of like a high-tech agarose gel. It will give you a good indication of your 28S/18S ratio for RNA (this is used to determine the overall RIN - RNA integrity number*) or your fragment size distribution for DNA.\n\n\n\n\n\n\n\nFragment analysis logistics:\n\n\n\n\n\nYou will most likely find one of these instruments as part of a service offered by a genetics/genomics facility within your University or Institute. They are not part of the standard benchtop lab equipment, as they require training to use and have ongoing maintenance. You should run Qubit first to get the concentration of your sample, then generally you will submit around 2-3 ŒºL to a technician who will run the fragment analysis for you (they may need to dilute the sample and will ask for the concentration). All three instruments above essentially function the same, but have different throughput capacity. The cost to running a sample is more than Qubit and NanoDrop; e.g., for one Bioanalyzer run, which can take up to 11 samples, this is around NZD$100 in consumables (one chip). The Fragment Analyzer and TapeStation have higher throughput, which can make it cheaper per sample if you have many, or the technician may plan to run your samples at the same time as other people‚Äôs samples on one multiwell plate. The run will take approximately 1 hour, and results will then be available immediately (the technician will likely email a pdf file to you). You can read the electropherogram in a similar way to an agarose gel; there are many guides online to help you e.g., see here Interpretation of Bioanalyzer Traces from the University of Rochester Genomics Research Center. Note the results will likely include an estimated concentration value - it is best to not use this value going in to library prep. Stick to the Qubit results.\n\n\n\nTogether, these will give you an almost complete picture of your sample, ready for sequencing. Note that if your samples are below threshold quality, there are library prep protocols that can allow for lower quality or degraded samples. Ideally, this should only be done if there is no option to re-extract, re-purify or repeat the experiment.\n\n*RNA integrity number: note that for some protosome species (e.g., molluscs, arthropods) the 28S rRNA subunit has a hidden break, which can negatively affect the RIN calculation. This results in the quality of the RNA appearing to be is worse than it actually is.\n\n\nHandy tip! It is best to run nanodrop and qubit immediately after sample extraction, to avoid freeze-thaw cycles. Put a small aliquot (2-3 ŒºL) of your sample aside too, ready to submit for fragment analysis.",
    "crumbs": [
      "Preparing for sequencing"
    ]
  },
  {
    "objectID": "preparing-for-sequencing.html#sequencing-libraries",
    "href": "preparing-for-sequencing.html#sequencing-libraries",
    "title": "Preparing for sequencing",
    "section": "Sequencing libraries",
    "text": "Sequencing libraries\nThe first thing that needs to be done for Illumina, PacBio or Oxford Nanopore (ONT) sequencing is to turn the DNA or RNA sample into something called a library. This converts the raw nucleic acids into a form that the sequencer can actually read. This is generally done by the technician who will also sequence your samples, but some labs may do library prep in-house (i.e., you may do it yourself!). For RNA samples, the Illumina and PacBio platforms require that the RNA is first converted into DNA during the library prep process. ONT has the added advantage that you can sequence native RNA directly, or you can convert it to cDNA for sequencing. DNA library preparation for ONT sequencing is also generally simpler than for Illumina and PacBio. This is because the technology used to sequence the DNA/RNA using ONT is quite different to how Illumina and PacBio achieve it. There are pros and cons to all three technologies‚Äìmore on that in the next section on flow cells and sequencing platforms 101.\n\nHandy note: You can think of one library as being equivalent to one sample. It is possible to make multiple libraries from a single sample, but it is not often done this way.\n\nLibrary preparation generally involves the following steps, and takes 1-2 days in the lab for Illumina and PacBio, and half a day to 1 day for ONT:\n\n\n\n\n\n\nIllumina lib prep molecular workflow diagram:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nONT direct cDNA lib prep molecular workflow diagram:\n\n\n\n\n\n\nImage from link here.\n\n\n\nis it too convoluted to try and cover all kinds of library prep in one general methods here??\n\n(Optional) Target enrichment / RNA selection\nSelectively enriching for your target molecule (e.g., for standard RNAseq you likely will do polyA capture). Total RNA library prep with ribo-depletion is also an option. For DNA, is somethign else approproaite here or is it covered in later steps?\n(RNAseq only) Converting your RNA into cDNA using reverse transcriptase. ONT also allows for direct RNA sequencing; see note below.\n\nFragmentation (if needed) Some kits fragment RNA before reverse transcription, others fragment cDNA/DNA after.\nThis ensures fragments are the right size for the platform (e.g., ~200‚Äì400 bp short reads for Illumina). Long-read platforms (ONT and PacBio) generally do not require fragmentation, but some size selection or shearing may be performed.\nEnd repair and A-tailing\nEnds are cleaned up so adapters can be ligated efficiently.\nAdapter ligation\nAdapters are short sequences that enable library binding to the flow cell (Illumina or ONT) or capture for SMRTbells (PacBio).\nSome kits combine this with indexing.\n\n\n\n\n\n\n\nPacBio lib prep - SMRTbell adapters\n\n\n\n\n\n\nNote: the SMRTbell technology allows HiFi (high fidelity) long read sequencing. The DNA becomes circularised, which allows the polymerase to make repeated passes around the DNA and the consensus sequence therefore has a higher accuracy than single pass sequencing.\n\n\n\n\nIndexing (barcoding)\nIndices allow multiple samples to be pooled together and sequenced on the same flow cell, then computationally (in silico) separated afterward (called mulitplexing and demultiplexing).\nSize selection / cleanup\nTypically done with magnetic beads to remove adapter dimers and select the desired fragment range.\nLibrary amplification (if required)\nSome protocols use PCR to enrich adapter-ligated molecules; others (e.g., some PacBio) are PCR-free. ONT has a cDNA-PCR sequencing protocol for lower input RNA samples.\nFinal QC and quantification\nUsing Qubit, Bioanalyzer/Tapestation/Fragment Analyzer, etc. This step ensures your library meets sequencing requirements.\n\n\n\n\n\n\n\nBioanalyzer trace of final library (Illumina mRNA lib prep)\n\n\n\n\n\n\nQ: What do you think the two large narrow peaks are around 15 bp and 1500bp?\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\nThe two narrow peaks are ladder sequence. These are internal standards of known size we add in for quality control. These can also be used for concentration estimation, but fluorescence-based quantification (e.g., Qubit) is more accurate.\n\n\n\n\n\n\n\n\n\nDirect RNA sequencing with ONT: Native RNA can be sequenced directly with ONT, which allows exploring of modified bases (e.g., methylated bases). It takes approximately 140 mins to complete library preparation. Currently, there is no option for multiplexing, although a multiplexing kit is scheduled for release in 2026.\n\n\n\n\n\n\n\nONT Direct RNA sequencing library prep workflow diagram:\n\n\n\n\n\n\nNote that the second complementary cDNA strand is synthesised for stability by reverse transcription. The cDNA strand is not sequenced, but improves the RNA sequencing output.\nImage from link here.\n\n\n\nThere are different library prep methods (i.e., protocols) for each platform that you will need to chose. This will depend on a few things, such as:\n\nThe quality of your RNA/DNA (e.g., high-quality RNA allows polyA selection; degraded samples may require ribo-depletion or specialised kits).\n\nThe species/tissue type you extracted your RNA/DNA from (e.g., plants have rRNA types that require plant-specific depletion kits, some tissues have high mitochondrial RNA content).\n\nThe type of analysis you want to do i.e., what is your research question.\n\nFor example, if you are doing a ‚Äògeneric‚Äô RNA sequencing project (e.g., you plan to do differential gene expression analysis to compare different samples), a common choice is Illumina stranded mRNA library prep, which uses polyA selection to capture mRNA. However, you may chose Illumina Stranded total RNA library prep with ribo-depletion, which is more expensive, but it has some advantages such as: it can capture non-polyadenylated RNAs (more comprehensive RNA profile) and and is a better option if your samples are partially degraded (it can also handle FFPE samples).\n\nTotal RNA library prep with ribo-depletion is the only option for prokaryotic RNA sequencing, as prokaryotes do not have polyA tails.\n\n\n\n\n\n\n\nExtra for the curious: The two main chemistries for Illumina RNA library prep.\n\n\n\n\n\nThere are two main versions of the Illumina RNA library prep kit chemistry. The ‚ÄúIllumina TruSeq‚Äù kit uses an older chemistry and the ‚ÄúIllumina Stranded‚Äù kit uses a newer chemistry. Within each of these versions are also several types of kits for specific purposes (e.g,. total RNA or mRNA). Both chemistries are very robust options and produce good quality sequence data. The TruSeq chemistry requires a slightly higher minimum mRNA input (100 ng) vs.¬†the minimum mRNA input for Illumina Stranded (25 ng), so if you have very low amounts of RNA you may need to use the ‚ÄúIllumina Stranded‚Äù kit. The newer ‚ÄúIllumina Stranded‚Äù also has the advantages of being a slightly faster protocol to complete, and allows for higher multiplexing (up to 384 samples, vs up to 96 samples for TruSeq). Note that both chemistries do strand specific sequencing. For most generic RNAseq projects, it won‚Äôt matter which one you use. You may note that some NZ sequencing facilities only offer one or the other chemistry.\nSee here for comparison between Illumina TruSeq stranded and Illumina stranded mRNA kits.\n\n\n\n\nDISCUSSION üí¨\n\n\n\n\n\n\nWhy do you think polyA capture is used for ‚Äòstandard‚Äô RNAseq?\n\n\n\n\n\nMature mRNAs have polyA tails, which can be selectively isolated using oligo DT coated beads that bind mature RNAs only. All other non-polyadenylated nucleic acids and cellular debris can then be washed away.\n\n\n\n\n\n\n\n\n\nWhy do you think we bother adding ‚Äúindex‚Äù sequences to libraries at all? Why not sequence each sample separately?\n\n\n\n\n\nIndexing (barcoding) allows multiple samples to be pooled in a single sequencing run. This massively reduces cost and time. Because each library carries a unique index, they can be mixed (i.e., pooled) together and sequenced simultaneously, and downstream software can computationally separate (demultiplex) them accurately afterward.\n\n\n\n\n\n\n\n\n\nWhy do you think size selection is such an important step? What would happen if we skipped it?\n\n\n\n\n\nSize selection ensures that fragments fall within the size range the sequencer expects. Without it, you may get:\n\nleftover adapter dimers (which waste sequencing reads as they take up ‚Äòreal estate‚Äô on the flow cell)\ntoo-short fragments (which cluster preferentially, causing over-representation and distorts the data e.g., sequencing may read into adapters or flow cell)\n\ntoo-long fragments (which may not fully sequence or reduce yield)\n\n\n\n\n\n\n\n&gt;&gt; How do I know if I have things like leftover adapter dimers in the final library??\n\n\n\n\n\nThe final fragment analysis (e.g., Bioanalyzer) trace of the library will show you. See below a picture of a trace with adapter dimers present (~120-150bp peak).\nTip: You can repeat the final wash step in the library prep protocol to remove adapter dimers and re-run Bioanalyzer to confirm. Sometimes a very small peak will still be present which will not overly affect the run (you can see one on the previous image above!)\n Image from Illumina general knowledge base\n\n\n\n\n\n\n\n\n\n\nFurther reading üìö\nAuckland Genomics provide great documention on choosing an approach for your sequencing project.\nDNA sequencing: whole genome, shotgun or metagenomic; which one is right for you?\nRNA sequencing: bulk RNA-seq, single cell RNA-seq, mRNA vs total, ONT RNA-seq; which one is right for you?",
    "crumbs": [
      "Preparing for sequencing"
    ]
  },
  {
    "objectID": "preparing-for-sequencing.html#flow-cells-and-sequencing-platforms-101",
    "href": "preparing-for-sequencing.html#flow-cells-and-sequencing-platforms-101",
    "title": "Preparing for sequencing",
    "section": "Flow cells and sequencing platforms 101",
    "text": "Flow cells and sequencing platforms 101\nThe three major sequencing companies are Illumina, Pacific Biosciences (PacBio) and Oxford Nanopore Technologies (ONT), which each produce their own platforms (i.e., the instruments or machines that perform the sequencing) and consumables (i.e., the flow cells and reagents). Each make different platforms that can handle different levels of throughput, but the chemistry and the ‚Äòreading‚Äô of the sequencing is the main point of difference between the three companies.\nA few examples of the different platforms are (throughput in brackets):\nIllumina\n\nMiSeq i100 (small)\n\nNextSeq 550 / 1000 / 2000 (medium)\n\nNovaSeq 6000 / X (large)\n\nPacBio\n\nSequel II\n\nRevio\n\nOxford Nanopore Technologies\n\nMinION (small)\n\nGridION (medium)\n\nPromethION 2 Solo / 2 Integrated / 24 (large - very large)\n\nA flow cell is the physical surface inside the sequencing machine (or platform) where the actual reading of DNA or RNA occurs. There are different sizes you can chose from, depending on how many reads you need. Although Illumina, PacBio, and ONT all call their consumables ‚Äúflow cells,‚Äù the underlying technologies are very different.\nFor the most part, and in-depth knowledge of how these flow cells work is not needed to get you started with your sequencing project. The more important thing to understand is the strengths and limitations of the different technologies and how to chose the right kind of sequencing for your research project.\nHere are the major differences between the technologies:\n\n\n\nFeature\nIllumina\nPacBio\nOxford Nanopore (ONT)\n\n\n\n\nHow sequencing works\nSequencing-by-synthesis (fluorescent nucleotides added one base at a time)\nSingle-molecule real-time (SMRT) sequencing (polymerase incorporates fluorescent bases inside Zero-Mode Waveguides (ZMW))\nNanopore sensing (changes in ionic current as DNA/RNA passes through a pore)\n\n\nFlow cell structure\nPatterned flow cell with billions of oligonucelotides that form clonal clusters\nSMRT Cell containing millions of ZMWs\nMembrane embedded with thousands of protein nanopores\n\n\nWhat binds to the flow cell\nLibraries bind via adapters to oligonucelotides ‚Üí amplified into clusters\nA single SMRTbell + polymerase complex loads into each ZMW\nDNA or RNA strand with a motor protein threads into a nanopore\n\n\nSignal detected\nFluorescent signal imaged each cycle\nFluorescent flashes when each base is incorporated\nChanges in electrical current across the pore\n\n\nAmplification?\nYes ‚Äî cluster generation required\nNo ‚Äî single-molecule sequencing. HiFi (high fidelity) reads are generated by multiple passes of the same circular molecule to create a consensus\nNo (PCR-free), though can use PCR in library prep\n\n\nTypical read length\n100‚Äì300 bp\n10‚Äì25 kbp HiFi reads\n10 kbp to &gt;100 kbp (ultra-long &gt;1 Mbp possible). Short reads e.g., 100bp also possible.\n\n\nCan sequence native RNA?\nNo ‚Äî convert to cDNA library\nNo ‚Äî convert to cDNA library\nYes ‚Äî direct RNA sequencing. Recognises standard and certain modified bases e.g., pseudouridine and m6A\n\n\nStrengths\nHigh accuracy; high throughput; cost-efficient; chemistry highly compatible with different species/tissues\nHighly accurate long reads; excellent for haplotype resolution\nLong and Ultra-long reads; portable; real-time analysis; can detect base modifications; enables adaptive sampling (unique software-based target enrichment or depletion method that provides multiomic view of the genome)\n\n\nLimitations\nShort reads only\nLower throughput than Illumina; expensive\nHigher raw error rate; pore lifetime limits yield and susceptible to clogging; short reads possible but less accurate and lower yield\n\n\nBest used for\nStandard RNA-seq; differential gene expression (DGE); de novo transcriptomes; high-depth short-read assays; metagenomics; error-correcting long reads\nGenome assembly (chromosome-level with HiFi); full-length RNA (Iso-Seq for isoforms); structural variant detection\nField-based sequencing (e.g., rapid microbial identification); genome assembly; native and full-length RNA including modification detection (e.g., methylation); isoforms detection as well as differential expression; structural variant detection",
    "crumbs": [
      "Preparing for sequencing"
    ]
  },
  {
    "objectID": "preparing-for-sequencing.html#decision-points",
    "href": "preparing-for-sequencing.html#decision-points",
    "title": "Preparing for sequencing",
    "section": "Decision points ü§î",
    "text": "Decision points ü§î\nChoosing a platform to do your sequencing comes down to the question you are trying to answer. See the last row in the table above ‚ÄòBest used for‚Äô for some examples of why you might pick one platform over another!\nWhen you get in contact with a sequencing facility, the question they will ask you is not how many samples are you sequencing, but rather, how many reads do you need? The number of reads you end up with per library will be approximately evenly distributed across all libraries, as they all get pooled together in equimolar amounts into one tube before loading on to the flow cell. The more libraries in the pool, the less reads per library. Hence, it does not matter how many samples you have (to an extent), what matters is how many reads you need in total. This will determine what size flow cell you need and which platform you will use, as different platforms have different capacity (e.g., Illumina NextSeq is a ‚Äòmedium throughput‚Äô platform, NovaSeq is a ‚Äòlarge throughput‚Äô platform). The number of reads you need per library scales with the size of the genome and/or complexity of your transcriptome.\nAs a general rule of thumb, for transcriptome sequencing you will need:\nTable 1: Reads per sample\n\n\n\nPurpose / Type\nApprox reads per sample\n\n\n\n\nGene expression profiling\n5‚Äì25 million\n\n\nComplete expression + alternative splicing\n30‚Äì60+ million\n\n\nDe novo transcriptome assembly\n~100+ million\n\n\n\nYou may be thinking that these values above are very broad ranges. How can you really know how many reads you need? The short answer is you don‚Äôt. You can use other publications as a guide and you can talk to other genomics people, but it is often an approximation. This section is here to give you a guide on how you can make a pretty good approximation of what you will need!\n\n\nThe next thing the sequencing facility will ask you if you are doing short-read sequencing (Illumina) is: do you want single-end or paired-end reads? This refers to whether you want a single read (read 1), sequenced from only one end of the library molecule (fragment), or if you want two reads per library molecule (read 1 and read 2, antisense and sense strands). Paired-end costs more, but gives you more resolution.\nThere are pros and cons to choosing either chemistry:\nTable 2: Single-end vs paired-end chemistry (short-read Illumina sequencing)\n\n\n\n\n\n\n\n\nChemistry\nPros\nCons\n\n\n\n\nSingle-end (SE)\nLower cost, fewer reads required (may be able to do more samples); sufficient for basic gene-level DGE\nLimited splice/isoform resolution; less confident mapping when mapping to a genome\n\n\nPaired-end (PE)\nBetter alignment; improved splice junction and isoform detection; more robust for complex transcriptomes or de novo transcriptome assembly\nHigher cost; ~2√ó sequencing required (two reads per fragment)\n\n\n\n\n\nLastly, if you are doing short-read sequencing (i.e., Illumina), the sequencing facility will ask you what read length you want. You can typically chose from between 50bp-300bp (platform-dependent). The choice between a lower or higher read length will be a balance of cost (higher read length = higher cost) and the level of information you need (complex, novel or de novo transcriptomes require higher read lengths; more straight forward analyses with well-annotated genomes can utilise lower read lengths). In contrast, for long-read sequencing platforms (ONT and PacBio), you do not specify a fixed read length. Instead, reads are generated as single, continuous sequences, and their length is determined by the size of the input molecules, the library preparation method, and the sequencing chemistry.\nChoosing a read length is a trade-off between cost, and the amount of information you will recover:\nTable 3: Read length (short-read Illumina sequencing)\n\n\n\n\n\n\n\n\nRead length\nPros\nCons\n\n\n\n\n50 bp\nLowest cost; highest sample multiplexing; sufficient for basic gene-level DGE in well-annotated genomes\nPoor isoform and splice junction resolution; higher multi-mapping\n\n\n100 bp\nGood balance of cost and information; reliable splice junction detection; widely used for standard RNA-seq\nSlightly lower throughput than 50 bp; may miss very complex isoforms\n\n\n150 bp\nImproved isoform resolution; better mapping across repetitive regions; useful for novel transcript discovery\nHigher cost; fewer reads per run\n\n\n300 bp\nMaximum per-read information; helpful for de novo transcriptome assembly\nRarely necessary for Illumina RNA-seq; expensive; reduced throughput\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE #1 üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (15 mins)\n\n\n\nExample RNAseq scenario:\nYou are working on a mouse model of cancer genomics. You want to do differential gene expression analysis to compare tumour samples to non-cancerous control tissue, to see if you can find genes that are up or down regulated in the cancerous tissue. You have 40 RNA samples, and since your species has a high-quality reference genome and annotation (i.e., mouse: Mus musculus), you know you have a ‚Äúgood‚Äù genome assembly and annotations to map your sequencing data back to (more on ‚Äúgood‚Äù genomes later!). The mouse genome is ~2.7 Gb (= 2,700,000,000 bp), diploid, with ~20,000 protein-coding genes.\nYou are particularly interested in alternative splicing and novel splice junctions, as you suspect that cancer-associated genes are often regulated at the isoform level.\nYou will sequence your samples at the Otago Genomics Facility, and see on their website they have an Illumina MiSeq and an Illumina NextSeq 2000. Use this Illumina benchtop sequencing platforms comparison guide to help you decide.\n\n\n\n\n\n\nWhich platform will you chose?\n\n\n\n\n\nThe Illumina NextSeq 2000 is a ‚Äòmedium‚Äô sized short read sequencing platform, ideal for standard RNAseq, and is well-suited to this project. It can output up to 540Gb.\nThe Illumina MiSeq is a ‚Äòsmall‚Äô sized short read sequencing platform, better suited to QC or amplicon sequencing, and can output up to 30Gb. It is too small for this project.\n\n\n\nNext you need to decide how many reads you need.\nBased on table 1 above, how many reads do you need per sample?\n\n\n\n\n\n\nReads per sample choice:\n\n\n\n\n\nYou decide you need 30 million reads per sample. You pick the lower end of the scale for ‚Äòalternate splicing‚Äô type sequencing, as you suspect the genes you are most interested in will be highly expressed, and don‚Äôt expect your tissue to have a particular high transcript diversity that would require more reads.\nYou work out what minimum output you need from the flow cell:\n30 mil reads * 40 samples = 1200 million reads (1.2B).\n\n\n\nNow you need to decide if you want paired-end or single-end reads.\nBased on table 2 above, what would you pick?\n\n\n\n\n\n\nPaired-end vs single end choice:\n\n\n\n\n\nPaired-end sequencing will give better detection of novel isoforms and splice junctions.\nThis doubles the number of reads generated per fragment and will give you better resolution.\n\n\n\nYou now need to decide which read length you need. Given the mouse genome is well-annotated, but you are looking for potentially novel isoforms, which read length would you pick, based on table 3?\n\n\n\n\n\n\nRead length choice:\n\n\n\n\n\n50 bp would be a good choice for a well-annotated genome like mouse for standard RNAseq, but does not suit this experiment, as you are looking for novel isoforms/alternative splicing.\n100bp is probably the best choice, balancing cost with novel isoform discovery.\nYou could also chose 150bp, if you want to be sure you‚Äôd capture novel isoforms, especially if they are quite long or complex genes - and don‚Äôt mind a higher cost.\n\n\n\nYou are now ready to pick your flow cell. Check out the Illumina NextSeq2000 flow cell specifications and chose which flow cell will suit this project best. There are four flow cells in ascending output size you can choose from: P1, P2, P3 and P4.\n\n\n\n\n\n\nWhich flow cell will you pick?\n\n\n\n\n\nChoices from earlier:\n\nTotal reads needed for your experiment: 1.2 billion reads.\n\nPaired end sequencing using 100bp read length (i.e., 2 x 100bp)\n\nCalculation: 1.2 billion reads x 100 (bp) x 2 (PE) = 240 Gb output is needed (i.e., 240 gigabase pairs, or 240 billion individual DNA bases sequenced).\nFlow cell options:\n\nP1 ‚Üí No 100bp option and way too low\n\nP2 ‚Üí 80 Gb (too low for 1.2B demand)\n\nP3 ‚Üí 240 Gb (fits 1.2B reads requirement exactly - best choice!)\n\nP4 ‚Üí 360 Gb (oversized for this project)\n\nNote: the maximum output stated for the flow cell is under optimal conditions, so the P3 flow cell choice just fits, but it is possible you will have less reads then anticipated (e.g., instead of 30 mil reads per sample, you may end up with 28 mil per sample).\n\n\n\n\n\nCongratulations! You are now ready to sequence your RNAseq samples.\n¬†\nNow let‚Äôs look at DNA sequencing in more detail.\nDNA sequencing may refer to whole genome, whole exome, or targeted sequencing approaches.\n\n\n\n\n\n\nFILL IN HERE\n\n\n\nmore explanation by someone who knows DNAseq better!\n\n\nFor Genome sequencing you will need:\n\n\n\n\n\n\n\n\n\nGenome size\nExample species\nApprox genome size\nApprox reads per sample\n\n\n\n\nTiny\nVirus\n&lt;0.1 Mb\n0.1‚Äì0.5 million\n\n\nSmall\nBacteria\n5 Mb\n5‚Äì10 million\n\n\nMedium\nYeast\n12 Mb\n10‚Äì20 million\n\n\nLarge\nFruit fly (D. melanogaster)\n175 Mb\n50‚Äì100 million\n\n\nVery Large\nHuman\n3 Gb\n600‚Äì1,200 million\n\n\nHuge\nWheat\n16 Gb\n6‚Äì12 billion\n\n\n\nWhere Mb = megabase pairs (i.e., 1 Mb = 1,000,000 bp)\n\n\n\n\n\n\nFILL IN HERE\n\n\n\nCoverage. talking about average coverage - really repetitive regions can have very low covergae, other more easily resolved sections will have higher coverage.\nheterozygosity homozygosity.\nhaplotypes etc.\nStick mostly to decision points - not a huge lecture/tutorial on genome/ DNA biology. Assume learners haev a genetics background - they just have not yet translated that knowledge into a practical application of NGS.\n\n\n\n\n\n\n\n\n\n\nEXERCISE #2 üß†üèãÔ∏è‚Äç‚ôÄÔ∏è (10 mins)\n\n\n\nExample DNAseq scenario:\n\n\n\n\n\n\nFILL IN - THIS EXERCISE NOT COMPLETE\n\n\n\nProbably needs one or two more ‚Äòdecision point‚Äô exercises for learners.\n\n\nYou are want to de novo assemble the genome of the New Zealand swamp maire (Syzygium maire), a critically-endangered, endemic myrtaceae species, which is under threat from the pathogen myrtle rust.\nBased on genome sizes of related Syzygium and Myrtaceae species (e.g., Syzygium aromaticum ~370 Mb, S. grande ~405 Mb), you expect the S. maire genome to be on the order of ~350‚Äì400 Mb.\nDe novo genome assembly requires long read sequencing to resolve the longest contiguous sequences possible (ideally the full chromosome length, but that is often not easy or possible to achieve!).\n\n\n\n\n\n\nWhich long-read sequencing platform should you chose?\n\n\n\n\n\nThere are two long-read platforms ‚Äì PacBio and ONT. PacBio better for throughput (amount of reads you can get back) and for accuracy than ONT. FILL IN.\n\n\n\nThe S. maire genome was published in 2024 by Balkwill et al. 2024. Tree Genetics & Genomes. The authors also used Illumina sequencing in this paper. What did they use it for? Why do you think they chose to do Illumina sequencing for these samples rather than PacBio?\n\n\n\n\n\n\nSOLUTIONS (collapse this once complete):\n\n\n\n\n\n\n\n\n\nFILL IN - THIS EXERCISE NOT COMPLETE\n\n\n\n\n\n\nAnswer: The used Illumina sequencing for 30 x samples at low coverage for resequencing.\nAnswer: The reason you would chose Illumina over PacBio for this application is cost and accuracy at the SNP level. They didn‚Äôt need to resolve full genomes as that was not needed for their research question for those samples.\n\n\n\n\n\n\n\nNote 1: Batch variation. Are you doing all your samples in one batch, or will you have multiple batches? Technical variation can occur, so you may want to wait and do all samples at once on one larger flow cell, or make sure you randomise samples across sequencing batches.\n\n\nNote 2: DNAseq vs RNAseq. You may have noticed we call it ‚ÄòRNAseq‚Äô, even though we convert the RNA into DNA before sequencing! By convention, this is still called RNAseq, to differentiate it from true DNAseq. ONT can directly sequence the RNA without converting it to DNA first; we call this native RNA sequencing.",
    "crumbs": [
      "Preparing for sequencing"
    ]
  },
  {
    "objectID": "preparing-for-sequencing.html#nz-sequencing-facilities-and-services",
    "href": "preparing-for-sequencing.html#nz-sequencing-facilities-and-services",
    "title": "Preparing for sequencing",
    "section": "NZ sequencing facilities and services",
    "text": "NZ sequencing facilities and services\n\nCurrent as of: January 2026\n\n\n\n\n\n\n\nNOTE\n\n\n\nnot planning to go into detail at all on non-NGS services around NZ. Sticking to high throughput genomics.\n\n\nThere are several dedicated sequencing facilities which offer NGS services around New Zealand that you can use. Most accept samples from any researcher, but there may be a higher cost for non-staff or students.\n\nOtago Genomics Facility (OGF)\nLocation: The University of Otago, Biochemistry Building, 710 Cumberland st.\nPerforms: Moderate to large scale RNAseq, DNAseq, amplicon sequencing, other sequencing (Illumina library prep and sequencing provided as a service; ONT platform requires the users to undertake the library and sequencing themselves with supported training and trouble-shooting advice).\nWebsite: Otago Genomics Facility\nPlatforms available:\n\nIllumina NextSeq 2000\n\nIllumina MiSeq\nONT P2 Solo\n\nONT MinION\nNanoString nCounter Analysis System (non-NGS)\n\n\n\nMassey Genome Service (MGS)\nLocation: Massey University, ADDRESS\nPerforms: Small scale RNA or DNAseq, single gene (Sanger) sequencing.\nWebsite: Massey Genome Service\nPlatforms available:\n\n2 x Illumina MiSeq\n\nApplied Biosystems 3500xl capillary instrumentation (non-NGS).\n\nNotes:\n\nOffers the Illumina TruSeq library prep method\n\n\n\n\n\n\n\nFILL IN MORE DETAILS\n\n\n\n\n\n\n\n\nAuckland Genomics\nLocation: ADDRESS\nPerforms: FILL\nWebsite: Auckland Genomics\nPlatforms available:\n\nIllumina FILL\nONT FILL\n\nNotes:\n\nOffers the Illumina Stranded mRNA library prep method\n\n\n\n\n\n\n\nFILL IN MORE DETAILS\n\n\n\n\n\n\n\n\nOther services and platforms around New Zealand:\n\nGenomNZ at AgResearch, Invermay are a commercial animal DNA genotyping laboratory, primarily for sheep, cattle, deer, goat and aquaculture sequencing and use an Illumina NovaSeq.\nLincoln have a MGI DNBSEQ-G400 genome sequencer, which is compatible with Illumina libraries. Unclear how or if researchers can use this\nCustom science (a supplier, does not do the sequencing for you) have negotiated the installation of a new PacBio Revio in Auckland\nBragato Research Institute are a grape and wine research institute in Blenheim, and have an ONT PromethION Sequencer which is available as a service to other research agencies and customers.\nGrafton Clinical Genomics perform Illumina and Ion Torrent next-generation sequencing, to support research, clinical and translational groups at the University of Auckland and Auckland DHB as part of the Academic Health Alliance relationship.\n\n\n\nWorking with NZ sequencing facilities\n\n\n\n\n\n\nFILL IN HERE\n\n\n\nstuff on submission process, show example form and brief explanation of how to fill out? I might have covered enough of this anywya by explaing libraries and reads etc.\nhow do you get data back from them ? how do they transfer it?\nDo all the seq facilities in NZ give you data back demultiplexed by default?\nDo anyone use basespace to share reads with people?",
    "crumbs": [
      "Preparing for sequencing"
    ]
  },
  {
    "objectID": "preparing-for-sequencing.html#international-sequencing-services",
    "href": "preparing-for-sequencing.html#international-sequencing-services",
    "title": "Preparing for sequencing",
    "section": "International sequencing services",
    "text": "International sequencing services\n\n\n\n\n\n\nFILL IN HERE\n\n\n\nInternational sequencing services - touch on this, why you may chose over NZ (or not chose!) then point towards the next section on ethical data management.\nShipping and storage logistics:\n\nRNA stability, and the need to preserve RNA (by freezing or other) for overseas sequencing?\n\nsilicon tubes that will keep the RNA good and reduce sequencing costs",
    "crumbs": [
      "Preparing for sequencing"
    ]
  },
  {
    "objectID": "test-styling.html",
    "href": "test-styling.html",
    "title": "test styling",
    "section": "",
    "text": "WORK IN PROGRESS",
    "crumbs": [
      "_test styles"
    ]
  },
  {
    "objectID": "test-styling.html#header-2",
    "href": "test-styling.html#header-2",
    "title": "test styling",
    "section": "Header 2",
    "text": "Header 2\ntext\n\nHeader 3\ntext\n\nHeader 4\ntext\n\nHeader 5\ntext",
    "crumbs": [
      "_test styles"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "WORK IN PROGRESS",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#getting-started-with-genomics",
    "href": "index.html#getting-started-with-genomics",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "Getting started with genomics",
    "text": "Getting started with genomics\nThis is a beginner-friendly workshop, designed to get you started with the world of genomics. Whatever you‚Äôre doing‚Äîwhether it‚Äôs transcriptomics, genome assembly, variant calling, metagenomics, or something else‚Äîif you will be using genomic data this workshop is for you!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "Prerequisites",
    "text": "Prerequisites\nLearners are expected to have a basic (undergraduate) level understanding of biological and genetic concepts, but no familiarity with genomics or bioinformatic/computational skill is required.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#whats-covered-in-this-workshop",
    "href": "index.html#whats-covered-in-this-workshop",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "What‚Äôs covered in this workshop",
    "text": "What‚Äôs covered in this workshop\n\nEthical data collection from a New Zealand perspective\nOrganisation‚Äîfrom messy lab books and excel spreadsheets to tidy, computer-friendly data\n\nWorking with sequencing facilities and understanding genomic data types\n\nData storage repositories and public services and facilities\n\nQuality control, wrangling of raw reads and an introduction to genomic terminology",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#whats-not-covered-in-this-workshop",
    "href": "index.html#whats-not-covered-in-this-workshop",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "What‚Äôs NOT covered in this workshop",
    "text": "What‚Äôs NOT covered in this workshop\n\nBasic descriptions of biological and genetic concepts.\n\nTraditional sequencing and services (e.g., Sanger sequencing, qPCR, genotyping, probe-based applications such as microarrays and NanoString nCounter).\n\nGenomic analysis workflows. We have multiple dedicated workshops for genomic pipelines; see our portfolio here.\n\nUsing shell or other bioinformatic tools. See our workshops on Introduction to shell and Introduction to R to get you started on this.\nUnderstanding the cluster, HPC resourcing and specialised software (e.g., we do not cover schedulers such as SLURM, partitions/CPUs/GPUs, choosing compute allocation allowance). See our workshop on Introduction to Bash Scripting and HPC Job Scheduler for this.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nAdapter\nShort synthetic DNA sequence ligated to the DNA molecule during library prep which allows the molecule to bind to the flow cell during sequencing and also provides a primer binding site\n\n\nbp\nBase pair\n\n\nDGE\nDifferential Gene Expression (analysis)\n\n\nHCS\nHigh Capacity Storage\n\n\nHPC\nHigh Performance Computing\n\n\nHiFi\nHigh fidelity (PacBio)\n\n\nIndex\nAlso known as a barcode. Short unique sequence added to each DNA molecule in one library, allowing the identification of that library/sample and thereby enabling pooling of the libraries for sequencing (one run = cheaper).\n\n\nGb\nGigabase pair (1,000,000,000 bp)\n\n\nGB\nGigabyte (file size / storage size)\n\n\nMb\nMegabase pair (1,000,000 bp)\n\n\nMB\nMegabyte (file size / storage size)\n\n\nMultiplexing\nSequencing multiple samples simultaneously in one run by combining libraries into one pool. Samples (i.e., libraries) are de-multiplexed (separated) in silico usually by the technician, based on unique indices.\n\n\nNGS\nNext-generation sequencing\n\n\nONT\nOxford Nanopore Technologies (sequencing company). Often referred to as ‚ÄúNanopore‚Äù.\n\n\nPacBio\nPacific Biosystems (sequencing company)\n\n\nResequencing\nSequencing part of an individual‚Äôs genome in order to detect sequence differences between the individual and the standard genome of the species. Often performed to detect SNPs, genotypes, variants.\n\n\nSE / PE\nSingle-end / Paired-end\n\n\nSMRT\nSingle molecule real time (PacBio)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "Attribution",
    "text": "Attribution\nThis workshop was developed by Dr Chlo√© van der Burg for the Genomics Aotearoa Bioinformatics Training Programme.\nParts of this workshop were re-used or adapted from The Carpentries Data Carpentry lessons on Genomics.\n\n\n\n\n\n\nCopyright information:\n\n\n\n\n\nAll Carpentries instructional material is made available under the Creative Commons Attribution license CC BY 4.0. The material in this workshop is not endorsed by the Carpentries and has been adapted by Genomics Aotearoa for our own teaching purposes.\nIn this workshop, the following lessons were adapted from The Carpentries Data Carpentry in the manner stated below:\n\nOrganisation and tidy data section has re-used material from Data Carpentry: Project Organization and Management for Genomics.\nXXX section here has has re-used material from Data Carpentry: Data Wrangling and Processing for Genomics. Currently not using this workshop but likely to re-use.\nThis workshop has been adapted from the general workflow of the Data Carpentry: Genomics Workshop Overview workshop, which includes the above lessons.\n\n\n\n\nMaterial in this workshop was also re-used from our other Genomics Aotearoa workshops, which includes:\n\n\n\n\n\n\nGenomics Aotearoa workshops:\n\n\n\n\n\nNOTE: Some of these workshops include attribution to other source materials, see the attribution notices enclosed within.\n\nMaterial from our Introduction to Shell workshop is re-used in the Genomic data wrangling and processing section.\n\nMaterial from our Introduction to R workshop - section on working with spreadsheets is re-used in Organisation and tidy data.\n\n\n\n\nDiagrams and images were also re-used in this workshop from online reference material, as follows:\n\n\n\n\n\n\nReference material:\n\n\n\n\n\n\nIllumina library prep molecular workflow diagram in Preparing for sequencing from Illumina Stranded mRNA Prep, Ligation Data Sheet. M-GL-02143 v1.0\n\nBioanalyzer trace of final mRNA library in Preparing for sequencing from Illumina Stranded mRNA Prep, Ligation Protocol Document # 1000000124518 v04.\n\nPacBio SMRTbell adapters image in Preparing for sequencing from Template Preparation and Sequencing Guide P/N 000-710-821-13.\n\nAdapter dimer image from Illumina general knowledge base used in Preparing for sequencing.\n\n\n\n\nDefinitions:\n\nRe-used material: Almost word-for-word, including images, with minor wording or styling modifications.\n\nMinimally-adapted material: Inspired by stylistic choices and general workflow, but material is primarily developed by Genomics Aotearoa.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Genomic Data Carpentry (Aotearoa edition)",
    "section": "",
    "text": "Made with ‚ù§Ô∏è and Quarto",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "ethical-considerations.html",
    "href": "ethical-considerations.html",
    "title": "Before you begin: Ethical considerations",
    "section": "",
    "text": "WORK IN PROGRESS",
    "crumbs": [
      "Before you begin: Ethical considerations"
    ]
  },
  {
    "objectID": "ethical-considerations.html#data-collection-perspectives-in-aotearoa",
    "href": "ethical-considerations.html#data-collection-perspectives-in-aotearoa",
    "title": "Before you begin: Ethical considerations",
    "section": "Data collection perspectives in Aotearoa",
    "text": "Data collection perspectives in Aotearoa\nAll indigenous species in New Zealand, as well as some exotic species, are considered by MƒÅori to be taonga. This taonga status is often used in reference to an individual species, however a broader consideration of taonga species may include interdependencies with other species and may also include the wider ecosystem. Genomic data collected from a taonga species individual is considered an extension of the species it was collected from, and represents part of their whakapapa, with the whakapapa of a species encompassing a myriad of interrelationships.\nIt is important to gain an understanding of MƒÅori perspectives in relation to taonga species data, and to engage with the appropriate MƒÅori community at each stage of collection, processing, analysis and sharing of genomic data. Considered and sensitive engagement demonstrates the researcher‚Äôs respect for MƒÅori communities, who hold cultural obligations as kaitiaki to care for taonga species within their respective tribal area. In addition, culturally appropriate engagement supports researchers to maintain high ethical standards, which subsequently gains the respect and trust of said communities.\nYour University, Institute or Organisation will likely have its own policies, guidelines or frameworks on how to undertake consultation with MƒÅori that you need to investigate for yourself.\nHere we have a few resources to get you started with thinking about how to include MƒÅori perspectives in your genomic research project:\n\nGenomic research on Taonga species\nGuidelines for Genomic Research on Taonga Species‚ÄìTe Nohonga Kaitiaki (below-left). Effective engagement with MƒÅori should be practised at all levels of research, as outlined by this engagement framework from pg 23 (below-right).\n ¬†¬†¬†¬†¬† \n\n\nMƒÅori Data Sovereignty\nTe Mana Raraunga - the MƒÅori Data Sovereignty Network advocates for MƒÅori rights and interests in data and has several resources on their website on these principles. This is a broader description of engaging with data; not specific to genomic data.\n\n\n\nGovernment policy\nThe Ministry of Business, Innovation and Employment (MBIE) outline their Vision MƒÅtauranga policy for unlocking the science and innovation potential of MƒÅori knowledge, resources and people. If you are writing government-funded or Royal Society funded grants, you‚Äôll need to think about how to incorporate Vision MƒÅtauranga in your research plan.\n\n\nFurther training\nTraining opportunities are available for Indigenous peoples through the Summer internship for INdigenous peoples in Genomics (SING Aotearoa) and Indigidata Aotearoa wƒÅnanga.",
    "crumbs": [
      "Before you begin: Ethical considerations"
    ]
  },
  {
    "objectID": "ethical-considerations.html#international-agreements-principles-and-protocols",
    "href": "ethical-considerations.html#international-agreements-principles-and-protocols",
    "title": "Before you begin: Ethical considerations",
    "section": "International agreements, principles and protocols",
    "text": "International agreements, principles and protocols\n\nFAIR and CARE\nIn 2016, the FAIR principles were published in Scientific Data. These guidelines set to provide a standard through which to improve digital assets:\n\nFindability\n\nAccessibility\n\nInteroperability\n\nReuse\n\nPart of conducting FAIR research is having a reproducible research pipeline. One way this can be done is by documenting your work on a version control system like GitHub. (See our GA workshop for more on Reproducibility with Git and Quarto).\nOpen science advocates have embraced this FAIR framework, while others critise its lack of protection of data, for not considering the rights and interest of those that should hold governance over data, in particular, Indigenous peoples. In response to this, the CARE framework has also been proposed by Global Indigenous Data Analysis, to complement the existing FAIR principles:\n\nCollective benefit\n\nAuthority to control\n\nResponsibility\n\nEthics\n\n\nThis story ‚ÄúOpen with care; Indigenous researchers and communities are reshaping how Western science thinks about data ownership‚Äù is an interesting read on these perspectives, and importantly how both sets of principles can be implemented together.\n\n\nNagoya protocol\n\nThe Nagoya Protocol on Access to Genetic Resources and the Fair and Equitable Sharing of Benefits Arising from their Utilization to the Convention on Biological Diversity is an international agreement which aims at sharing the benefits arising from the utilization of genetic resources in a fair and equitable way.\nThe Nagoya Protocol is a supplementary agreement to the Convention on Biological Diversity. It covers the fair and equitable sharing of benefits arising from the utilisation of genetic resources. New Zealand is not a signatory to the agreement due to the overriding importance of the Treaty of Waitangi in domestic affairs (source: Te Nohonga Kaitiaki).\n\n\nCITES species\n\nCITES is the Convention on International Trade in Endangered Species of Wild Fauna and Flora. It is an an international agreement between governments. Its aim is to ensure that international trade in specimens of wild animals and plants does not threaten the survival of the species.\nSome species are therefore subject to certain controls for international trade and require specific authorisation to import, export, re-export or introduce. You can search the species list on Species+.\nNZ has not ratified or accepted CITES, and as such NZ‚Äôs status is ‚Äòaccession‚Äô i.e., they have not signed the Convention.",
    "crumbs": [
      "Before you begin: Ethical considerations"
    ]
  },
  {
    "objectID": "ethical-considerations.html#discussion",
    "href": "ethical-considerations.html#discussion",
    "title": "Before you begin: Ethical considerations",
    "section": "DISCUSSION üí¨",
    "text": "DISCUSSION üí¨\n\nIf sample collection involves species that are Indigenous to Aotearoa, consultation and potential involvement of kaitiaki, and perhaps the wider community, will be required. How will you identify the appropriate kaitiaki and/or community to engage with?\nWorking with MƒÅori communities (and indeed any community) should begin with fostering relationships so that trust is built between researcher and community. What actions could you take to build positive relationships with kaitiaki and community? Conversely, if you have made mistakes that in hindsight may reflect poor practice, please feel free to share these.\nWhat retrospective actions could you take if data has been collected without kaitiaki and community consultation or involvement, and it should have been?",
    "crumbs": [
      "Before you begin: Ethical considerations"
    ]
  }
]